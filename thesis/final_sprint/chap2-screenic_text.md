# Screenic Text #

# Text as Interface/Text as Process #

The command-line interface (CLI), once a culturally universal site of intersection between human and digital process, has found itself virtually superceded by the visually metaphoric instrumentation of the graphical user interface (GUI). The mechanism of this transition from CLI to GUI within mainstream computing was the introduction of Microsoft Windows into the ecosystem of IBM-compatible PCs. The result was the injection of an additional semiotic layer, charged with a new modality of visual signification, between the user and the hardware (Stephenson 1999). For almost two decades consumer versions of Windows, however, were "DOS front-ends" that could not function without real, historical dependencies fulfilled by the presence of DOS deep within the guts of the operating system. Windows 1.0, for instance, used DOS's file operation functions (_Windows 1.0_ 2010). This dependency on DOS recedes over time, eventually disappearing entirely in Windows XP, in which the DOS interface and functionality still exists but has migrated out of the substrate and into a virtual machine (_Windows XP_ 2010).

The roots of the command line lie in a very physical process: the teletype. A teletype is resembles a typewriter in that it presented the users with a standard typewriter keyboard as a control. Pressing a key would result in an inked stamp of that keys respective character smacking onto the paper and retract, leaving its mark. Simultaneously the triggering of the key might be punched into a tape as a binary sequence representing the character. If so, the control was thus separated intrinsically between human and digital---it was not, as in todays keyboards, simply electrical signals converted into numbers transparently beneath our fingertips but rather also a physical instantiation of the sequence on a paper strip. The screen of this human-digital intersection was instantiated on the same paper as the recording of the input, using the same ink and stamps but now powered by the response of the machine to its human input.

Stephenson identifies an extremely formal dynamic of interacting through teletypic screens he encountered when learning to program in high school:

  Anyway, it will have been obvious that my interaction with the computer was of an extremely formal nature, being sharply divided up into different phases, viz.: (1) sitting at home with paper and pencil, miles and miles from any computer, I would think very, very hard about what I wanted the computer to do, and translate my intentions into a computer language--a series of alphanumeric symbols on a page. (2) I would carry this across a sort of informational cordon sanitaire (three miles of snowdrifts) to school and type those letters into a machine--not a computer--which would convert the symbols into binary numbers and record them visibly on a tape. (3) Then, through the rubber-cup modem, I would cause those numbers to be sent to the university mainframe, which would (4) do arithmetic on them and send different numbers back to the teletype. (5) The teletype would convert these numbers back into letters and hammer them out on a page and (6) I, watching, would construe the letters as meaningful symbols. (Stephenson 1999) 

In this text, titled ¨In the beginning was the command line,¨ Neal Stephenson proceeds to identify the underlying mechanisms of human-digital processual intersections: ¨computers do arithmetic on bits of information. Humans construe the bits as meaningful symbols.¨ He notes, however, that this act of translation is increasingly obscured by ever-increasing metaphoric abstraction, starting with the GUI and carrying on over the course of the evolution of graphical interfaces. Command-line interfaces are close to the bottom of the ¨stack¨ of the cross-translation between symbols and bits, whereas ¨[w]hen we use most modern operating systems, though, our interaction with the machine is heavily mediated. Everything we do is interpreted and translated time and again as it works its way down through all of the metaphors and abstractions¨ (Stephenson 1999). 

Text, as the least abstracted of the available sites of symbol translation between digital binary forms (which can be considered the text of a different alphabet) and human process, is the formal level of computing. As such, the general non-consideration of the command-line interface in new media discourse is a disservice to the metamedium with which much of that discourse concerns itself.

One of the angles by which the CLI approached--tangentially--are discussions of code poetry. Florian Cramer provides a step by step engagement with computation both within digital contexts and written language that leads to an important insight:

  The cultural history of computation shows that it is as rich and contradictory as that of any other symbolic form. It encompasses opposites, algorithms as a tool versus algorithms as a material of aesthetic play and speculation, computation as inner workings of nature (as in Pythagorean thought) or God (as in Kabbalah and magic) versus computation as culture and a medium of cultural reflection (starting with Oulip and hacker cultures in the 1960s), computation as a means of abolishing semantics (Bense) versus computation as a means to structure and generate semantics (as in Lullism and Artificial Intelligence), computation as a means of generating totality (Quirinus Kuhlmann) versus computation as a means of taking things apart (Tzara, cut-ups), software as ontological freedom (GNU) versus software as ontological enslavement (Netochka Nezvanova), ecstatic computation (Kuhlmann, Kabbala, Burroughs) versus rationalist computation (from Liebniz to Turing) versus pataphysical computation as the parody of both rationalist and irrationalist computation (Oulipo and generative psychogeography), algorithm as expansion (Lullism, generative art) versus algorithm as constraint (Oulipo, net.art), code as chaotic imagination (Jodi, codeworks) versus code as structured description of chaos (Tzara, John Cage).
  
This contradictory nature envelops the command line as well. A tool at once more powerful and more flexible yet equally more opaque and unyielding. To begin to understand the command-line is to begin shooting lit arrows in the dark, lighting fires of process that will burn the results of their functioning onto your harddrive, your graphics card, your BIOS, or your network as onto your screen. The Unix command

`rm -rf /*`

will erase the entire contents of that Unix´s filesystem from the hard drive. The code for `rm` loaded into memory survives to delete itself from a core component of its materiality, that is, the raw 1s and 0s on the magnetic platters that constitute the persistent body of the command. It will not, however, survive the reboot inevitably awaiting such a fubar´d system.

## Remediation and the Command Line ##

While the modern command line may be a remediated teletype machine, it is crucial to note that the commands available at the prompt _re-mediate nothing_. The processes embodied in file operation commands, for instance, instantiate into material effects on hard drives. They are abstractions of processual hybridization that results in the same command in the same operating system having the same effect, in this instance, on the file system. The modules loaded into the assemblage offering this abstraction depend on the format of the file system (NTFS, HFS+, ext*, etc.), the motherboard-to-disk controller protocol (IDE, SCSI, SATA, etc.) and the driver specificities of that disk controller. All of these elements are unique, digital assemblages. The _embodied processes_ that are typed commands cannot be accurately held to the standard of a theory that is based on a conception of mediums as containing and extending previous mediums. 

These commands, these arrows flaming into the dark, have only the output of text in order to satisfy the needs of _immediacy_. There are interactive commands, to be sure. Text editors and email clients are two commonly abstracted interfaces. However, do to the natural reliance of the keyboard as the control of the command line, these interfaces are as likely to rely on combinations (or "chords") of key presses for purposes of navigation and process instantiation. This is a new type of immediacy only available within the electronic writing space, an immediacy that comes with a steep learning curve but which--once mastered--rewards the user with productive potentials beyond what is accomplishable through the semiotic abstractions of the GUI. Some hackers joke that their favorite operating system is their text editor `emacs`, which is noted for accomplishing everything from coding to typesetting (through the powerful AUCTeX extension) to emails and calenders to web browsing. All interaction is accomplished through these chords of key presses. The learning curve requirement means that "so much needs to be filled in" by the user: commands are clearly 'cool,' in the language of Marshall McLuhan (1964).


Neatly obscured and packaged as a 'command,' processes such as the aforementioned `rm` are _nothing_ if not process. Commands are either latent or instantiated process. 

# Language is Programming #

As N. Katherine Hayles puts it, "screenic text and programming are logically, conceptually, and instrumentally entwined" (Hayles 2004: 80). 

In their book _The Alphabetization of the Popular Mind_, Ivan Illich and Barry Sanders write of the dimensionality that phonetic alphabets introduce to words. For them, "language" does not proceed our capacity to store representations of words as sounds:

  The historian misreads prehistory when he assumes that "language" can be spoken in that word-less world. In the oral beyond, there is no "content" distinct from the winged word that always rushes by before it has been fully grasped, no "subject matter" that can be conceived of, entrusted to teachers, and acquired by pupils (hence no "education, "learning," and "school"). For it is the record in phonetic writing that first carries what is heard across a chasm separating two heterogeneous eras of speech. The alphabetic scribe carries what is spoken from the ever-passing moment and sets down what he has heard in th permanent space of language. Only with this act can knowledge, separate from speech, be born. (Illich and Sanders 1988: 7-8)

Though their phrasing is inflammatory in its assured rejection of the possibilities of "knowledge" and "learning" amongst oral societies, they do however illustrate the new dimensions of potential arising from the intersection of words and the phonetic alphabet.  The encoding of the _sounds themselves_ into writing transforms "the page into a mirror of speech," freezing "the flow of speech itself onto the page" (11, 13). For Illich and Sanders, the resulting new dimensions delivered to speech through this synthesis of speech and alphabet into language include nothing less than "knowledge," "education," and even "logic" itself. Though Illich and Sanders do not use the language of Simondon's ontogenesis, we can map how, as these dimensions begin to fill and expand over time, the metastability of the system continuously fluctuates as new potentials are described and, through that description, affect the landscape upon which further potentials unfold. For instance, in 1492 language becomes recognized as an avenue of control as the Spanish royalty begin the project of implementing a standard language to consolidate their subjects into a more rule-able assemblage. 

Illich and Sanders note the lineage of Orwell's Newspeak, a product of the utopian writerly tradition of positioning language as the means of subjugation (in dystopias) (110). Through Newspeak the power of the State supplants the exercising of power by elites--the power of the State is exercised _on them_, rather than by them--as "the State has turned into a book that is constantly rewritten" (111). Illich and Sanders, however, use the term Newspeak to refer to "an approach or an attitude that treats language as a system and a code" (112). Fitting directly into this phrase, then, is cybernetics theory. 

While digital processes necessarily change the understandings and uses of any word used to describe them, it is important to note that for the most part the words of computer science are appropriations from language that existed before computers. For instance, the word 'program' was first applied to computers in 1945 to refer to the "act of expressing an operation in the terms appropriate for the performance of a computer" (Illich and Sanders: 113). Thus, the word was appropriated from physical schedules of performances by human beings in an event and mapped onto the logospace of computational performance.

If the ¨so-called 'language' of physics is a code, a system of signs, a formal theory, an analytic tool that derives part of its value from its near-independence from ordinary speech" (Illich and Sanders: 116), then the `language´of the command-line fits this description as well, with one further caveat: _text on the command-line is kinetic_.

Florian Cramer provides an important perspective when he notes ¨the technical principle of controlling matter through the manipulation of symbols, is the technical principle of computer software as well¨ (Cramer 2005: 16). This manipulation of symbols underpins the entirety of the digital assemblage, and in that way the digital reflects humanity´s relationship with language. In an earlier essay called ¨Digital Code and Literary Text,¨ Cramer identifies a privileged relationship between language and binary code (Cramer 2001). 

The connection to magic is instantiated culturally in the language of the hacker class. Firstly, in the formulation of the individual command-line entry as an  `invocation.´ This recognition is important, for text has never been so kinetic as it is on the command line. From the literally typed input of the teletype machines to the virtual terminals with transparent backgrounds and multi-aphabet encoding running in a GUI, the textual input of the command-line represents a site of language that promotes words from their status as _signifiers of meaning_ to become _signifiers of action_. Not merely the description (evocation) of action, but the literal _invocation_ of action through words.


# Language adds dimensions #

Language can be conceived as a 'program' for decoding the strings of symbols we call an alphabet into the meaning those symbols were arranged in order to convey. Language is thus adding the dimension of meaning to the digital code of the alphabet. Contrary to the popular misconstruing of 'digital' with 'binary', the term 'digital' just means a system with discrete units capable of formally representing some _thing_. For instance, alphabets are utilized for calculations in some fields of informatics. Likewise, characters of the alphabet are used to stand in for entire mathematical algorithms in the case of physics. Due to the discrete jumps between characters, they are utilizable for computation as well as representation (one could even argue that there is a computation occuring in the decoding of the representation from signifier to signified). 

A good example to demonstrate this aspect of the alphabet is what is known as hexadecimal notation. Rather than our familiar base-10 (decimal) system of representing numbers, hexadecimal is a base-16 system. Counting from 1-9 is the same as in base-10, but starting at 10 we run into the difference. Being base-16 means that any given column in a numerical sequence must "count" 16 times (including 0). So 10-15 are represented in hexadecimal by the alphabetic characters A-F. Hexadecimal is a common notation in computation due to its extreme translatability back and forth between binary while at the same time maintaining a more compact system of representing numbers than either binary or decimal notation. Whether one would consider such utilization of the alphabet for representing numbers constitutes a remediation is a question that points to the fragility of the remediation concept: by focusing on 'media', the term becomes useless to discuss appropriation between anything that is not considered a medium. That is, it invites increasingly expansive definitions of 'medium' or risks blocking itself from application in a diverse range of instances. Processual hybridity, however, is a means for denoting this intersection of digits and alphabet, thus providing a handle by which to grasp hexadecimal notation in a critical context without resorting to materialist theories.

In the words of Florian Cramer, the "alphabet of both machine and human language is interchangeable, so that 'text'--if defined as a countable mass of alphabetical signifiers--remains a valid descriptor for both machine code sequences and human writing" (2001). The difference lies in syntax and semantics---that is, "computer algorithms are, like logical statements, a formal language and thus only a restrained subset of language as a whole" (Cramer 2001). In recognizing this fact, note that syntax and semantics also influence the dimensional modulation of language in its application to systems. Whereas "language" as a whole can be conceived as a program for decoding 'alphabetical signifiers' into meaning, the subsets of language known as programming languages undergo a process of _transcoding_. Formulated as action from its first instance, words in computer code are _signifiers of digital process_ rather than meaning. (Or, in the case of "codeworks" and code poetry, the meaning of the words is a meaning constructed of references to both the signifieds of both human language and digital process).

The relationship of code to language is that of a subset constrained by the specificities of syntax (Cramer 2001). The digital computer is ruled by syntax, which could be considered the defining means of mediation between digital computers and human processes. These processes include the actions of the users, the objects created/stored/distributed/displayed on digital computers, _and the processes by which these digital operations are instantiated_.

¨Computation and its imaginary are rich with contradictions, and loaded with metaphysical and ontological speculation. Underneath those contradictions and speculations lies an obsession with code that executes, the phantasm that words become flesh. It remains a phantasm because again and again, the execution fails to match the boundless speculative expectations invested in it.¨ (Cramer 2005: 125).

# Top Down, Bottom Up #

## WYSIWYG ##

WYSIWYG, meaning "What You See Is What You Get," is a mode of interface design in which operations are performed in an extremely top-down manner. In terms of typesetting, the definitive example of WYSIWYG is Microsoft Word. Word can be deemed a remediation of the typewriter. However, it may be useful instead to consider it as an _appropriation of the grammar_ of the typewriter. That is to say, rather than speak of it in terms of 'remediation,' which makes less and less sense the more digitally-unique features are added to the interface of Word. What does our knowledge that Word remediates the typewriter add to a discussion of WYSIWYG?

WYSWIWYG is clearly an instance of an attempted _immediacy_--by remediating the then-familiar modality of the typewriter into the context of the computer screen. But the entire concept of media seems complicated by this remediation: if the typewriter is a medium, something which few media theories would likely argue against, then is Microsoft Word then a medium as well? This begins a slippery slope: would not all applications become, or have some claim to be considered as, media?

To avoid this I would argue that what is called remediation is, rather than media "consuming and extending" previous media, a dynamic in which the _grammars_ of one processual hybridity (the keys-stamps-ink-paper assemblage of the typewriter) are appropriated and _re-conditioned_ by another process (MS Word). 

The WYSIWYG interface is clearly a top down approach, as all manipulations come from invoking processes onto what has already been placed into the interface.

## Processed Text ##

Processed text comes in two flavors: _semantic_ and _formal_. Semantic formats such as HTML and XML are far more widely used than formal formats such as TeX (and LaTeX/ConTeXt). While both are bottom up in contrast to WYSIWYG, there is a distinction even here between top down and bottom up. (They are both bottom up in that the typesetting goals of any block of text are specified at the beginning of that block, i.e. `<em>` and `\quote' are both typed before the block of text begins.

HTML is top down, however, because that is its rendering model. By imbuing blocks of text with semantic qualities, one abstracts away the process of displaying those semantic blocks. Order is imposed from above, both through Cascading Style Sheets (CSS) and through the rendering algorithm of a given browser's implementation.  