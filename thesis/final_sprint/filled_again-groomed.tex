%\enableregime[utf]  % use UTF-8

\setupcolors[state=start]
\setupinteraction[state=start, color=black, contrastcolor=black] % needed for hyperlinks

\usemodule[simplefonts]
\setmainfont[Linux Libertine O]
\setmonofont[inconsolata][11pt]

\setuppapersize[A4][A4]  % use A4 paper
%\setuplayout[width=middle, backspace=1.5in, cutspace=1.5in,
%             height=middle, header=0.75in, footer=0.75in] % page layout
%\setuppagenumbering[alternative=doublesided, location={header,margin}]  % number pages
\setuppagenumbering[alternative=doublesided, location=]
\setupbodyfont[12pt]  % 12pt font
\setupwhitespace[medium]  % inter-paragraph spacing

\setupinterlinespace[line=18pt] % should be 3/2 spacing

\setupcaptions[headstyle=smallcaps, style=\tfx, way=bytext, prefixsegments=none]

\setupindenting[medium]
\indenting[always]

\setuphead[section][style=\sc, alternative=margin]
\setuphead[subsection][style=\sc, alternative=margin]
\setuphead[subsubsection][style=\sc, alternative=margin]

\setupheadertexts [text]
	[{\sc \getmarking[chapter]}]	[{\pagenumber}]
	[{\sc \getmarking[Doctitle]}] [{\pagenumber}]	

% this is for truly empty pagebreaks
\definepagebreak
  [mychapterpagebreak]
  [yes,header,right]
\setuphead
  [chapter]
  [page=mychapterpagebreak, header=empty, footer=empty]

% setup title page
\unprotect
\definemarking[Author]
\definemarking[Doctitle]

\def\doctitle#1{\gdef\@title{#1}\marking[Doctitle]{#1}}
\def\docsubtitle#1{\gdef\@subtitle{#1}}
\def\author#1{\gdef\@author{#1}\marking[Author]{#1}}
\def\date#1{\gdef\@date{#1}}
\date{\currentdate[day, month, year]}  % Default to today unless specified otherwise.


\setupcolor[hex]
\definecolor[titleRed][h=910A00]

\def\maketitle{
  \startalignment[center]
    \blank[force,2*big]
      {\scd \color[titleRed]{\@title}}
		\blank[big]
			{\scb \@subtitle}
	\stopalignment
	\startalignment[flushleft]
    \blank[25*big]
		\starttabulate[|l|l|]
    			\NC Name: \NC \@author
			\NR \NC Student Number: \NC 6100473
			\NR \NC Email: \NC john.haltiwanger@gmail.com
			\NR \NC Website:	\NC http://drippingdigital.com/
			\NR
		  \NR	\NC Date: \NC \@date
			\NR \NC Supervisor: \NC Richard Rogers
			\NR \NC Second reader: \NC Geert Lovink
			\NR \NC Institution: \NC Universiteit van Amsterdam
			\NR \NC Department: \NC Media and Culture (New Media)
		\stoptabulate
		\blank[3*big]
		\starttabulate[|l|]
		\NC {\sc Keywords} \NR
		\stoptabulate
		medium theory, generative design, ontogenesis, transduction, process oriented perspective, typesetting
\stopalignment}
\protect


% define descr (for definition lists)
\definedescription
  [descr]
    [headstyle={\bf},style=normal,location=top, hang=20,
  width=broad,
	command=\hskip-1cm,margin=1cm]

% prevent orphaned list intros
\setupitemize[autointro]

% define defaults for bulleted lists 
\setupitemize[1][symbol=1][indentnext=no]
\setupitemize[2][symbol=2][indentnext=no]
\setupitemize[3][symbol=3][indentnext=no]
\setupitemize[4][symbol=4][indentnext=no]

\setupthinrules[width=15em]  % width of horizontal rules


% define a special head type of bibliography
\definehead		[bibliography] [chapter]
\setuphead		[bibliography] [number=no, page=mychapterpagebreak]
%\definecombinedlist		[content][chapter,section,bibliography]
\setuplist		[bibliography] [headnumber=no]

\definehead		[intro]	[chapter]
\setuphead		[intro]	[number=no, page=mychapterpagebreak]
\definecombinedlist		[content][intro,chapter,section,subsection,subsubsection,bibliography]
\setupcombinedlist		[content][alternative=c,interaction=all]
\setuplist		[intro]	[headnumber=no]

% let's get pretty chapters
\def\MyChapterCommand#1#2% #1 is number, #2 is text
  {\framed[frame=off,bottomframe=on,topframe=on]
     {\vbox{\blank\headtext{chapter} #1\blank#2\blank}}} % \vbox is needed for \blank to work
\def\MyEmptyChapterCommand#1#2% is a comment necessary?---apparently so...
	{\framed[frame=off,bottomframe=on,topframe=on]
			{\vbox{\blank#2\blank}}}

\setuphead[chapter][command=\MyChapterCommand, style={\scc},page=mychapterpagebreak,header=empty]

\setuphead[bibliography][command=\MyEmptyChapterCommand, style={\scc},page=mychapterpagebreak,header=empty]

\setuphead[intro][command=\MyEmptyChapterCommand, style={\scc},page=mychapterpagebreak,header=empty]

\setupheadtext[chapter=Chapter] % used by \headtext


% for block quotations
\unprotect

\startvariables all
blockquote: blockquote
\stopvariables

\definedelimitedtext
[\v!blockquote][\v!quotation]

\setupdelimitedtext
[\v!blockquote]
[\c!left=,
\c!right=,
before={\blank[medium]},
after={\blank[medium]},
]

% for long quotes
\definestartstop
  [longquote]
  [before={\indenting[never]
    \setupnarrower[left=0.5in,right=0.5in]
    \startnarrower[left,right]},
  after={\stopnarrower
    \indenting[yes]}]

% for bibliographic entries

% following hanging indent code (also in workscited) taken from 
%  http://www.ntg.nl/pipermail/ntg-context/2004/005280.html
% [NTG-context] Re: Again: "hanging" for a lot of paragraphs?
%  ~ Patrick Gundlach
\def\hangover{\hangafter=1\hangindent=0.5in}
\definestartstop[workscited][
  before={
    \page[no]
    \indenting[never]
    \startalignment[left]
    \bibliography{Bibliography}
    \stopalignment
    \setupwhitespace[medium]
    \bgroup\appendtoks\hangover\to\everypar
    },
  after={
    \egroup
    \indenting[yes]}]

\protect

\definestartstop
  [abstract]
  [before={\blank[4*big]
					 \midaligned{\sc Abstract}
           \startnarrower[2*middle]},
   after={\stopnarrower
          \blank[big]}]

\setupheader[state=start]


\starttext
	\doctitle{Grammars of Process}
	\author{John C. Haltiwanger}

\docsubtitle{Interrogating Media Through the Conditions of Generative Design}


%\setuppagenumbering[location=]
\setupheader[state=stop]
\maketitle
\page

%\setuppagenumbering[alternative=doublisided, location={header,margin}] 
\page[odd]
\startabstract
Despite years of theorization, a concise definition of what constitutes a medium remains elusive. Theorists have variously described media as extensions of human senses (Marshall McLuhan), as agents of reform ruled by a double-logic of remediation (Jay David Bolter and Robert Grusin), as aggregates of material specificity (N. Katherine Hayles), and as evolutionarily selected forms defined by their effects (Lev Manovich). While all of these theorists use examples of and from specific media, none of them explicitly address the specificities of media themselves. This thesis proposes a {\em process-oriented perspective} that seeks to address this slipperiness that has resulted from unclear definitions of the concept---a slipperiness that only intensifies within the context of the computer metamedium. Media are seen as reflexive sites in which humans create grammars that organize and distribute processes. As reflexive sites, they both change and are changed by human beings. The reflexiveness of the computer metamedium, defined as it is by its programmability, inspires an investigation into the dynamic and co-evolving relationship of human and digital process. The practice of {\em generative design} is selected as an evocative instance of this relationship, and a reflexive engagement with the practice is undertaken as the thesis becomes a site of generative typesetting. Undergoing this practice-based approach leads to complications with existing theorizations of media. The concept of process utilized here is organized according to Gilbert Simondon's theory of {\em ontogenesis}, a framework that questions \quote{becoming} rather than \quote{being} and in so doing provides a mechanism for explaining collective change.
\stopabstract
\page[odd]
\setupheader[state=start]

\placecontent

\startfrontmatter
\intro{Acknowledgments}

The final shape of this thesis is deeply indebted to those who have
helped me along the way. First I would like to thank Richard
Rogers. Without his gentle-yet-forceful pressure to elevate the
discussion contained within my thesis, I fear that the project
would ultimately demonstrate little of whatever theoretical power
it currently enjoys. Likewise, without Geert Lovink the project
would not exist in the first place, as it was his request for a
post-journal publishing platform that started me on this quest of
generative typesetting for multiple output formats. Thanks also to
Florian Cramer, not least for explaining that simply typesetting
the thesis in two formats was enough technical work for a
single-year masters thesis but also for providing a strong
perspective from which to begin. I am deeply indebted to Michael
Jason Dieter for introducting me to the work of Gilbert Simondon,
without whose theory this thesis would not be feasible. Huge thanks
go to Femke Snelting and Pierre Huyghebaert for their indispensible
interview and for welcoming me into their extremely creative
network. For issues with JavaScript in the HTML version of this
thesis, Michael Murtaugh once again proves his potency as a
programmer. Special thanks to my colleagues at the UvA, for helping
me to stay sane in the midst of an insane project: Hania
Pietrowska, Natalia Sanchez, Rakesh Kanhai, Sarah Moore, Morgan
Curry, Marc Stumpel, Allison Guy, and Ramses Petronia. Every day I
grow to further realize that without my friends, I am nothing.
Finally, a sincere thanks to all those who have shared insights and
conversations with me as I pushed to develop my ideas further.

And last, but never least, my family. Thank you mom, dad, Brett,
Julia, and Ann. Thank you for your unwavering support!

\intro{Introduction}

Today's new media theory increasingly invokes {\em materiality} as
a significant, perhaps even {\em the} significant, mode of
investigating digital objects and the media through which they are
delivered. This thesis questions such a centrality of materiality
through a practice-based, process-oriented approach. {\em Process}
is proposed as the atomic unit of that which new media theory
investigates. This is true on a formal material level: applications
run as either as individual process or as assemblages of process
which are managed by an operating system and through which the
application's code is accomplishes all of its tasks, from memory
and access to algorithmic execution on the central processing unit.
A process-oriented approach will be shown to provide an alternative
methodology for engaging with and understanding software-based
media that may be more productive than what material sepcificity or
effects analysis currently provide. Process also allows a fresh
perspective for examining human-digital relations. Human processes
and digital processes are seen as inextricably intertwined and
inhabiting the same {\em metastability}, leaving any discussion of
digital process that excludes relevant dimensions of human process
necessarily unfinished.

Process has recently elevated as a focal point within the design
world as more and more designers switch to, or otherwise integrate,
generative workflows. {\em Generative design} is a form of design
in which software algorithms are used from the bottom up, through
source code that directs all drawing and manipulation of the
objects the source describes. This is opposed to the top-down, What
You See Is What You Get (WYSIWYG) style of design embodied in the
industry-standard applications from Adobe. New media theorist
Florian Cramer has identified generative design as the cutting edge
of design in the Netherlands (Cramer, Monsoux, and Murtaugh 2010).
The emergence of generative design as a widespread practice is
reflected by the Breda-based Graphic Design Museum's decision to
host an installation of generative works called {\em InfoDecoData},
as well as a symposium of the same name.

This space of generative design provides an ideal site for
investigating questions of materiality and medium-specificity
within the computer {\em metamedium} for a number of reasons. The
importance of source code to generative design intersects with an
on-going dialog within software studies concerning the position of
source code in a study. Generative design's position close to the
cutting edge of what is being done with computers invites inquiry
into the processes that are assembled to compose today's computers.
Finally, the tendency of generative design to involve the
command-line interface compels an investigation into this
under-theorized medium, and with it the sustained primacy of text
within computer interfaces. This primacy of text is reflected all
the way from source code to the labels on buttons and menus in a
GUI interface.

Before explaining in more detail the process-oriented perspective
utilized by this thesis, however, it first seems prudent to visit
some of the dominant strains of medium theorization found in the
history of media studies which will be interrogated in this thesis.

\section{Marshall McLuhan and the Temperature of Media}

Marshall McLuhan's prominence in media studies is in no small part
related to the pionering role he had in shaping the field.
Beginning with his work in analyzing advertising, published in 1951
as {\em The Mechanical Bride}, McLuhan critically integrated the
language of an advertising industry that had developed a vocabulary
for considering their new role in targetting not just print but
also television and radio. This critical integration allowed for a
consideration of media on a level that had not, up to that point,
emerged. That is to say, it was his seminal work
{\em Understanding Media} that proposed that it is the study of
{\em media themselves}, and not simply their content, that is
necessary for critical engagement.

One of the means he proposed as a distinguishing characteristic
between media was the \quotation{temperature} of a given medium.
Though the wording is perhaps somewhat un-intuitive, McLuhan
defines \quotation{hot} media as those media forms which extend
\quotation{one single sense in \quote{high definition}} (McLuhan
1964: 24). The effect of such extension is a reduction in the
involvement of the audience to the medium. A photograph, for
instance is \quotation{hot,} while a cartoon is \quotation{cool}
because it contains relatively sparse amounts of visual
information. The phonetic alphabet, McLuhan argues, is a
\quotation{hot and explosive} medium, with vastly different effects
than the cool medium of ideogrammic writing (25). In his view, he
transformation of this alphabet by the printing press, this
\quotation{hotting-up} of the writing, led to
\quotation{nationalism and the religious wars of the sixteenth century}
(25). The temperature of a medium reflected the ratio of its
extension to perception. Hot media are those in which the ratio of
extension for a sense overwhelms other senses.

The temperature of a medium interacts with the temperature of a
culture in a way that necessarily redefines the culture (and, it
could be argued, the medium as well, which may decrease in
effective temperature as the temperature of a culture rises in
relation). For instance, the
\quotation{hot radio medium used in cool or nonliterate cultures has a violent effect, quite unlike its effect, say in England or America, where radio is felt as entertainment}
(33). For this and similar views, Mcluhan received condemnations as
a \quote{technological determinist,} someone who argues that
technology defines culture.

Such a position, however, is complicated by the simple fact that
McLuhan viewed mediums as extensions of human senses.
\quotation{All media are extensions of some human faculty---psychic or physical}
(1967: 26). As extensions of the human, any changes imbued by media
into culture necessarily maintain a root cause in the human
themselves. This dynamic is asserted in this thesis as well, which
views media as reflexive sites that develop in a state of dynamic
equilibrium between humans and our media.

McLuhan also developed a pedagological tool he called the Tetrad.
The tetrad is means of examining media and its effect on society.
Phrased as four questions, by its very phrasing it proposes a sort
of physics for media, a description of a dynamic through which all
media undergo upon their development and introduction.

\startlongquote
- What does the medium enhance?
- What does the medium make obsolete?
- What does the medium retrieve that had been obsolesced earlier?
- What does the medium flip into when pushed to extremes? (McLuhan 1988: 
\stoplongquote

This is a departure from pure techno-determinism because in the end
it is human processes which develop new media forms and embed older
forms within them---it is human processes that expect or demand
that new media \quotation{masquerade} as older media. And,
ultimately, it is human processes that the medium is designed to
enhance.

\section{Bolter and Grusin's Double-Logic of Remediation}

Jay David Bolter and Richard Grusin take this question of what
actually drives the physics described by the Tetrad. Their
conclusion is a form of double-logic that envelopes one of the key
elements of the Tetrad---that new media forms embed the older ones
they replace, a process which they termed {\em remediation}---and
explains it as the result of two interacting forces,
{\em immediacy} and {\em hypermediacy}. The logic of immediacy
seeks to erase the \quotation{medium-ness} of a medium. This can be
seen in the drive for \quotation{transparent} interfaces:
\quotation{a transparent interface is one that erases itself, so that the user would no longer be aware of confronting a medium, but instead would stand in an immediate relationship to the contents of a medium}
(Bolter and Grusin 1996: 318). Hypermediacy, on the other hand,
\quotation{acknowledges multiple acts of representation and makes them visible}
(328). Hypermediacy is, then, an opposing vector, one that delights
in highlighting the presence of the medium rather than attempting
to obfuscate or \quotation{disappear} it. The root cause of this
process of double-logic, in the end, is the desire to
\quotation{achieve the real}:

\startlongquote
Hypermedia and transparent media are opposite manifestations of the same desire: the desire to get past the limits of representation and to achieve the real. They are not striving for the real in a metaphysical sense. Instead, the real is defined in terms of the viewers experience: it is that which evokes an immediate (and therefore authentic) emotional response. Transparent digital applications seek to get to the real by bravely denying the fact of mediation. Digital hypermedia seek the real by multiplying mediation so as to create a sense of fullness, a satiety of experience, which can be taken as reality. (Bolter and Grusin 1996: 343) 
\stoplongquote

Since Bolter and Grusin position mediations themselves as both real
and representations of the real and remediation as the
\quote{mediation of mediation,} then remediation reforms reality
(346). As media are {\em ipso facto} sites of mediation, media can
be seen as reformative agents that are constantly reconfiguring
reality. Mediation and reality thus become \quote{inseparable,} in
a constant state of mutually reflexive reconfiguration. While their
work provides a useful system for describing the dynamics of media,
still missing is a solid definition of what constitutes a medium.
In the case of the computer metamedium, the elusiveness of a
definition for media will prove problematic for the accurate
application of their framework.

\section{N. Katherine Hayles and Medium Specificity}

In her essay
\quotation{Print is Flat, Code is Deep: The Importance of Media-Specific Anaylsis,}
N. Katherine Hayles presents a more technical, or at least
{\em specifiable}, definition of media. Calling for a renewed focus
on materiality, Hayles presents evidence that literary hypertext
exists in printed books as well as within its familiar context of
the computer. Both printed and screenic hypertext demonstrate
{\em medium specificities}---that is, in the process of becoming
embodied within one or the other materialities, literary hypertext
displays characteristics that are specific to those materialities.
Medium-specific analysis thus
\quotation{attends to both the specificity of the form\ldots{}and to citations and imitations of one medium in another}
(Hayles 2004: 69).

Hayles sees materiality as an \quotation{interplay} between
physical traits and strategies of signification,
\quotation{a move that entwines instantiation and signification at the outset}
(67). This formulation, though in different words, moves beyond a
specified division of form from content into a holistic definition
that sees those components as intrinsic and inseparable. This flies
in the face of certain ideas surrounding web design, which
holds---in its workflow if not also in its underlying
conception---that form and content are separable and distinct.

Hayles' utilizes her media-specific analysis to demonstrate how the
specificities of print and screen necessarily shape literary
hypertexts through materially driven constraints. Her conception of
media, though not explicit, apparently revolve around this concept
of specificity. If there were no differences in the specificities
of print and screen, then there would be no material differences in
the texts she describes. Though Hayles admirably conceives of
materiality as an amorphous interplay between physicality and
process, the focus on specificities creates potential problems when
moving into media specific to the computer. If specificities are
used to determine media, and specificities may change even within
different versions of a single program, where do we cease to apply
to label \quote{medium}? Once again the elusiveness of a definition
potentially short-circuits the application of Hayles' framework to
computer-specific media.

\section{Lev Manovich and Media Hybridity}

Lev Manovich's {\em Software Takes Command} is unique in this group
in that the theories it presents are specific to the computer, or
at least to {\em metamedia}. Manovich first develops a concept of
{\em cultural software} based on the premise that the software has
become the \quotation{new engine of culture} (2008: 11). Software
finds itself in this position thanks to the status of the computer
as a metamedium, defined as such by its programmability. This
programmability allows the computer to not only simulate other
media, it also provides a space for the creation of entirely new
media.

Manovich defines these new media according to their
{\em hybridization}, a process for which he uses evolution as a
metaphor. From this perspective, media hybrids are created
constantly. After their creation they go through a process of
selection and either \quotation{replicate} or die off after failing
to propagate (90). Manovich is careful to state that he is
\quotation{not making any claims that the actual mechanisms of media evolution are indeed like the mechanisms of biological evolution}
(90).
\footnote{This thesis provides a provisional elucidation of these mechanisms
in providing a historical account of programmer interfaces through
the lens of Gilbert Simondon's transduction.}
Successful media hybrids go on to become
\quotation{basic building blocks,} ready for combination with other
hybrids.

As a first example, Manovich presents the \quote{image map,}
described as
\quotation{a successful combination of media \quote{genes}} (90).
Combining hyperlinks with an image, image maps hybridize both
techniques of hypertext and techniques of image manipulation.

\startlongquote
When designers start attaching hyperlinks to parts of continuous images or whole surfaces and hiding them, a new "species" of media is born. As a new species, it defines new types of user behavior and it generates a new experience of media. (Manovich 2008: 91)
\stoplongquote

This phrasing provides a rough version of what, to Manovich,
defines a medium: specificities of interaction and experience. If
an image map is a new species of media, then it logically follows
that it is a medium in its own right. The underlying mechanism that
thrusts image maps into this position is the representation of
hyperlinks as hidden within a surface that becomes
\quotation{alive} and displays a continuum of hyperlinking in which
some parts of this surface are
\quotation{\quote{more} strongly hyperlinked than others} (91).
That the same effect (a surface with non-obvious internal linking)
can be achieved within traditional HTML by disabling the coloring
and underlining that typically denote hyperlinks seems to have
escaped Manovich's attention.

This thesis takes a divergent approach, one that would never
consider the image map as a specific medium. Rather, an image map
is conceived as a new processual grammar that hybridizes processes
of hypertext and static images to articulate a new domain of
interaction with the user. Whereas Manovich expresses a
hybridization of {\em media}, then, this thesis views such a
formulation problematic inasmuch as the concept of \quote{medium}
is increasingly vague.

\section{Process-Oriented Perspective}

In his outlining of the nature of a
\quote{processual media theory,} Ned Rossiter asserts that
\quotation{a processual media theory examines the tensions and torques between that which has emerged and conditions of possibility; it is an approach that inquires into the potentiality of motion that underpins the existence and formation of a system}
(2007: 178). While the argument that Rossiter builds in his chapter
on the subject revolves mainly around interrogating and the
processes that drive new media within its institutional settings,
the thrust of his argument---that new media empirics must
\quotation{reflexively engage with the field of forces that condition its methodology}---maps
easily to a more general line of inquiry (171). This is perhaps
best embodied in his explanation of a
\quote{processual aesthetics of new media}
\footnote{Where aesthetics is used
\quotation{to speak of the organization and management of sensation and perception}
(Rossiter 2007: 166).}:

\startlongquote
A processual aesthetics of new media goes beyond what is simply seen or represented on the screen. It seeks to identify how online practices are always conditioned by and articulated with seemingly invisible forces, institutional desires and regimes of practice. (Rossiter 2007: 174)
\stoplongquote

While a truly exhaustive investigation of the multitude of
relations---social, economic, technological, ideological---involved
in generative design is simply not within the scope of this thesis,
I believe that it is important to begin filling in the
\quotation{gap} between source code and execution. A
process-oriented perspective encourages this by first defining a
given materiality as an {\em assemblage of process}. This is as
true in the physical world as it is in the digital, as all that
exists has taken its shape as a result of {\em becoming}. From this
recognition, this process-oriented perspective integrates the
theoritization of ontogenesis proposed by Gilbert Simondon. This
theory will be described in further detail in a later section
(!WHICH!). For now, however, it is best to explain that in
Simondon's ontogenesis
\footnote{That is, the \quote{being of becoming.}},
change in a system is the result of incongruency within that
system. In other words, difference causes change.

To test this assertion, I survey historical developments within
computing as well as practices of contemporary generative design
and typesetting with open source software. Where do changes---or,
in Simondon's language, {\em transductions}---occur within the
domain of computation? How did the assemblages of process upon
which generative design rely {\em become} what they are today? The
historical survey demonstrates that changes in computers do arise
in response to \quotation{differences}---that is to say, problems.
These problems can be \quote{real}, \quote{virtual},
\quote{imaginary}, or otherwise. The solutions to these problems
are influenced by the structure of relations within which the
problems themselves arise. This observation demonstrates that
Simondon's description of ontogenesis is sound, as well as
applicable to describing the dynamics of the metamedium.

The second stage of the process-oriented perspective seeks to
interrogate and critique the mode of existence of a {\em medium}
within the computer {\em metamedium}. When materiality within the
computer is defined as \quote{assembled processes,} media lose
their seeming rootedness in material specificities---that is,
unless everything assembled within the computer is deemed a medium.
To make this point clear: if a medium is defined by its material
specificities, and within a computer every non-identical digital
object contains---or is embedded within---particular specificities,
then every unique digital object (or any unique application used to
handle it) has the potential to be labelled a \quote{medium}.
Variations in interface, say the difference between Windows
Explorer and Mac OS X's Finder can be said to hold real, material
specificities. Labelling them each as distinct \quote{media},
however, opens an un-closeable box through which every digital
process that maintains its own interface becomes a medium. In my
opinion, this is unacceptable.

A separate element, outside of material specificities, must be
invoked in order to explain and discuss media native to the
computer. In response to this question I identify the existence of
{\em grammars of process} which enable, require, and inspire the
assembling of process. Combined with a sufficiently conservative
definition of \quote{medium,} these processual grammars provide
space for discussing the vast variation in material specificities
without diluting the term \quote{medium} into a troubling
meaninglessness. While not entirely sufficient to resolve the
boundaries of medium and not-medium, this concept of processual
grammar nevertheless provides at least a tool for articulating and
guiding discussions along those lines.

\subsection{Attributes of Process}

Every digital process has, at its ultimate origin, a human. Saying
this is not an attempt to elide the latent unpredictability that at
times characterizes human-digital interaction. Rather it is an
attempt to highlight the human role in the human-digital
metastability. The rate of computation has increased the impact of
human-digital processes in that the outcomes of those processes are
delivered faster and faster. The results will either match the
intentions of the originating human process, or they will not. In
the second case we can find the first evidence of the effects of
digital process on human process:
{\em the code behind the digital process will be re-arranged in an attempt to deliver an output that satisfies the intention of the human processes.}
Whether this modulation of the executed code is through
sliders/input boxes/etc within a GUI interface or through direct
reworking of the source code itself, the effect is the same: the
code executed has been re-configured according to the goal of human
process. The result(s) of the digital process, experienced through
a screen, can match, exceed, or fail this goal. In turn, human
process is affected and the next move is made according to new
goals and/or revised digital processes.

Process is reflexive. It's outputs are \quotation{feedbacks} of
their inputs, reconfigured by the process. These outputs
reconfigure the metapotential in any given system. The reflexivity
of process has material effect. As it reflects the inputs into the
outputs, the outputs in turn reflect new (or else simply different)
potentials back into the context which is the reciprocal contact
point in which the processes began. This language is extrapolative
into any set of intersections. This paper considers just the subset
of human-digital recipricity, and within the relatively static
domain of typesetting.

A new configuration of metapotential in any system results in the
reconfiguration of (all) other systems as well. This fact reflects
the {\em fractal} nature of process---there is a degree of
non-reducibility inherent in any discussion of process, as
ultimately certain factors in its functioning are unknown to us.

Processes are organized, set in motion, and interacted with through
processual grammars. These grammars represent an organizing logic
surrounding processes, a logic that integrates these processes into
distinct {\em process hybridities}. In other words, grammars are
the means through which processes are assembled. An obvious example
of a processual grammar is the industrial assembly line: ruled by
an organizational logic that hybridizes processes of craftsmanship,
standardized units of measure and time, industrial fabrication
tools, and the logic of commodity, any manufacturing that is to
take place within an assembly line must organize according to the
grammar that shapes it.

Generative design is clearly ruled by grammars. Its foundational
body consists of source code, a medium known by its total reliance
on grammar for functionality.
\footnote{The positioning of source code as a medium is clarified in the
discussion of the evolution of programming.}
This thesis focuses on the grammars that enable generative design
through investigations into historical processes that resulted in
the contexts in which generative design occurs and into the process
itself through a reflexive methodology.

\section{Generative Design Begins With Words}

Generative design is a fruitful site for examining questions of
medium specificity and materiality for a number of reasons. That
its materiality is clearly a result of process is obvious when
considering the \quote{workspace} of much of generative design:
plain-text source files. These source files often, though not
exclusively, undergo their processing within a command-line
interface (CLI). The command-line interface is defined by the
primacy of text in its workings. The centrality of text to
generative design invites a corrective movement against a general
overlooking of the processes behind typesetting among new media
theory. While the surfaces of text and textual interfaces have been
investigated in numerous ways (Bolter 2001; Fuller 2000), there has
been a general lack of theoretical concern regarding the underlying
processes of {\em text placement} in the metamedium. Likewise, as
opposed to the overflowing amount of literature relating to
visually-rich computer interfaces, very little theory has been
written regarding the command-line---despite its place as the
historical interface (once contemporaneous with punch cards) by
which digital processes were initiated. Far from being obsolete,
both Microsoft and Apple ship command line interfaces within their
operating systems. In Microsoft's case, significant money has been
spent developing their modern command line implementation
Powershell. Additionally, Google recently found the command-line
relevant enough to release a CLI tool for interacting with its
online services (Holt and Miller 2010).

This centralization of text and the command-line raises pertinent
questions that may help to clear up the almost-hopelessly fuzzy
nature of materiality in the computer meta-medium. What aspects
define the materiality of the command-line? What are its
medium-specificities? Do existing theories such as
{\em remediation} apply to the workings of the command-line? What
differences exist between various command-lines?

Furthermore, what processes have assembled in order to form the
context of modern-day command-lines? Such a question delves into
the origins of the personal computer and its perceived significance
both before and after its introduction. In Alan Kay's vision of the
{\em computer as metamedium}, the system supports and encourages
the instantiation of new media forms by individuals who have no
formal background in programming (Kay 1977). Apple's Macintosh
famously delivered the vast majority of the human-computer
interface innovations developed by Kay's team at Xerox PARC
labs---without this key feature of easy programmability.
\footnote{In the video documentary series {\em Triumph of the Nerds}, Steve
Jobs reports that
\quotation{they showed me really three things. But I was so blinded by the first one I didn't even really see the other two}
(PBS 1996). In his explanation he was so excited about the
demonstration of PARC's graphical user interface that he was unable
to absorb the importance of either the Smalltalk--80
object-oriented programming environment or the Ethernet networking
technology.}
The orginal Macintosh operating system also presented its total
lack of a command-line as an ideal formulation.

When Apple introduced Mac OS X a decade and a half later, its Unix
underpinnings---complete with command-line---appeared in marketing
as a selling point. For almost a decade the adoption of Apple
computers has risen continuously. In chapter two, Mac OS X is
described as a particular hybridization of process that both
intersect with and reflect \quotation{external} processes such as
economics, inertia, and ideology.

\section{Processes Within the Borderland}

The centrality of source code itself---rather than the centrality
of the source code as text---to generative design provides a second
compelling reason for investigating materiality and
medium-specificity within the metamedium. Currently at issue within
the emerging field of software studies is where to responsibly
place source code in its investigations. Some propose, as Lev
Manovich does in his {\em The Language of New Media}, that
understanding the logic of new media requires investigating the
field of computer science for the
\quotation{new terms, categories, and operations that characterize media that become programmable}
(Manovich 2001: 48). This is a call to software as {\em logos}. It
attempts to solidify theory by giving it a specific direction---the
logic and objects of software.

Wendy Hui Kyong Chun, however, questions this direction,
criticizing the erasure of
\quotation{the vicissitudes of execution and the structures that ensure the coincidence of code and its execution}
that results when one elevates source code---and by extension
software---as a totalizing logic (Chun 2008: 303). When theorists
such as Alexander Galloway argue that source code is
\quotation{automatically executable,} they fetishize source code by
collapsing source code with the effects of that code's execution.
In other words, the execution itself is erased, along with the
conditions buttressing that execution. Rather than approach a
project of solidifying theory (that is, ending
\quotation{vapor theory} as advocated by Geert Lovink and Galloway,
among others) through reducing the computer metamedium to the code
that it runs, Chun advocates an approach of code as a
{\em re-source}, a perspective which
\quotation{enables us to think in terms of the gap between source and execution}
(321). This gap seemingly includes---or perhaps is---the
\quotation{borderland} in Hayles positions materiality,
\quotation{the connective tissue joining the physical and mental, the artifact and the user}
(Hayles 2004: 72). That Chun identifies the
\quote{code as re-source} perspective as positioning an
\quotation{interface as a process rather than as a stable thing}
resonates with the process-oriented perspective proposed in this
thesis (Chun 2006: 321).

Similarly resonant is the recent series of lectures by David
Crockford. While the series relates to the programming language
JavaScript, Crockford utilizes his entire first lecture to describe
the evolution of programming interfaces throughout the development
of digital computers (Crockford 2010a). Starting from the
\quotation{spaghetti code} of wires that provided the original
means of programming, Crockford proceeds to explain in great detail
the multitude of processes that defined programming prior to its
current state. Integrating these observations with the memoir
account of Severo Ornstein allows for an attempt to fill in
historical elements that belong to this gap, or borderlands
(Ornstein 2002). The historical evolution of these important sites
of interaction between human and digital processes is seen as an
overlooked aspect in both the field of computer science and in new
media discourse. The relevance of {\em inertia} in the composition
of human-digital processes is demonstrated by examining the
surprisingly large number of elements that remain a part of
computing that have no reason for present-day integration other
than the weight of history. This seems an important aspect within
the inter-relations of human and digital processes.

\section{Reflexive Methodology}

In addition to the historical survey of digital processes and its
attempt to map Gilbert Simondon's language onto that domain, this
thesis has a separate case study: the typsetting of itself, in
multiple output formats, through a generative workflow. This
provides a secondary mechanism for interrogating Simondon's theory
of ontogenesis. Central to ontogenesis is the question of
{\em individuation}.

By examining the underlying processes of presentation required to
\quote{typeset} the text itself, this reflexive methodology further
demonstrates the validity of classifying materiality as assembled
process. Through the utilization of FLoSS software, multiple output
formats will be not only be investigated but also materially
instantiated through a designed mechanism of process---a
{\em processual hybridity}. These output formats represent two of
the top formats currently used to manage and display texts
digitally: HTML and PDF. Questioning the materiality of the input
format, Markdown, is a useful exercise: what is the materiality of
a format whose use implies an intention to convert to---to exist
as---a multitude of {\em other materialities}? This type of
materiality (transitional, \quotation{unfinished}) runs throughout
the field of generative design. By designing, documenting, and
describing a genuine generative workflow I am able to integrate a
significant degree of reflexivity into the thesis at a level of
materiality as well as content.

The historical dynamics shaping the condition of this project
emerge as I discuss the specificities of the formats used in this
project. As the \quotation{content layer} of the World Wide Web,
HTML is the most pervasive, if not also the most well-known, markup
language on the planet. It is defined in its specification as What
processes does HTML hybridize in coming into existence? What
specificities does it entail? What processes has it in turn
inspired?

The second output format, PDF, is actually achieved through the use
of an intermediary format. This intermediary format is composed of
constructions for a process hybridity called {\CONTEXT}. In specific
terms, {\CONTEXT} is a macro package for the venerable typesetting
program called {\TEX} (this year celebrating its 32nd, or
2\letterhat{}8, birthday). {\CONTEXT} is a fast-moving project that
provides a unique site for assessing process hybridity.

\stopfrontmatter \chapter{Ontogenesis and Generative Design}

Dealing as it does with the nature of \quote{becoming} rather than
\quote{being,} Gilbert Simondon's theory of ontogenesis provides an
ideal framework from which to begin describing the operation of
generative design. Like all digital objects, a generative design
within the computer necessarily involves an assemblage of process
common to the metamedium---electrical current, computer memory,
cycles in the central processing unit, screens, etc. (It is
important to note that not all generative design takes place within
a computer, a fact that I return to in my overview of various forms
of generative design.)

Unlike many other digital objects, however, generative design
involves source code. This places it in a category with software,
which also remains rooted in source code. Source code is a medium
of great variability in its processual grammars. There are
literally thousands of programming languages out there, each
presenting a process hybridity assembled under a specific
organizing logic. The medium itself satisfies an urge for immediacy
by striving for an ever-increasing fluidity in expressing the
intentions of the programmer. One of the significant traits of this
medium is that the processes developed within it are abstract
expressions of intent that require a specific stage of translation
in order to realize that intent. (This stage of translation exists
for both software and \quotation{code works,} the latter being
translated by a mind rather than a compiler or interpreter.) This
triple-materiality---as programmer {\em intention}, abstract
{\em representation}, and executable {\em object}---of source
code-native processes naturally invokes the requirement of a
framework that describes becoming.

\section{The Transduction of Source Code}

In his text \quotation{The Position of the Problem of Ontogenesis,}
Simondon writes,

\startlongquote
By transduction we mean an operation--physical, biological, mental, social--by which an activity propagates itself from one element to the next, within a given domain, and founds this propagation on a structuration of the domain that is realized from place to place: each area of the constituted structure serves as the principle and the model for the next area, as a primer for its constitution, to the extent that the modification expands progressively at the same time as the structuring operation. (Simondon 2009: 11).
\stoplongquote

Repurposed from the language of chemistry and microbiology,
Simondon's word choice provides a visual metaphor of transduction
through the example of a substrate---swelling with
{\em metapotential}---that crystallizes. The final formation is the
substrate fulfilling this metapotential, a fulfillment that arises
only through an unpredictable unfolding involving emergent factors.
\footnote{The language of chemistry was likewise appropriated for the term
\quote{interface} (Cramer and Fuller 2008: 149).}

The model of Simondon's ontogenesis is built around the question of
{\em individuation}, a historically debated topic within
philosophy. Rather than focusing on the problematic of {\em being},
as the question of individuation was typically phrased in the West,
Simondon provides the much more useful observation that
individuation is rather a process of {\em becoming}. His philosophy
has only recently been translated into English, a fact that
demonstrates his notion of transduction: the translation
reconfigures the space of English-based theory through its
structural integration into theoretical frameworks, which in turn
modify the landscape in which new theory occurs.

This example of translation-as-transduction applies equally to
source-based processes, wherein the process of translation
represented by code compilation/interpretation results in an
executable object that modifies the computer metamedium by altering
its state of metapotential. As with theory, to what degree this
alteration occurs depends on the utility and dissemination/adoption
of the software.

\subsection{Elements of Ontogenesis}

Simondon's ontogenesis relies on several key concepts. The first is
the idea of a {\em metastable equilibrium}. The lack of
understanding of a metastable equilibrium, Simondon claims, is the
reason why individuation has
\quotation{not been able to be adequately thought out} (2009: 6).
Prior to the scientific formulation of a metastable equilibrium
defined by multiple energy states and transitions between these
states, the concept of equilibrium was tied to stability. A stable
equilibrium cannot host \quote{becoming} because it is
\quotation{the equilibrium that is reached in a system when all of the possible transformations have been realized and no more force exists}
(Simondon 2009: 6). Simondon's individuation is the result of a
resolution of a metastable system, following the logic of the
crystallization of substrates---the substrate is a metastable
equilibrium that undergoes transformation as a result of its own
composition.

The second, especially valid for the domain of generative design
and---specifically---the workflow of this thesis, is the concept of
the {\em preindividual}. The preindividual regime is
{\em \quotation{more than unity, and more than identity}} (6;
original emphasis). Simondon articulates this, as is his fashion,
by referring to the as-yet scientifically incompatible dynamics of
quantum and wave mechanics, both of which articulate specific
realities about photons but neither of which explain the
{\em reality} of the photon. Simondon proposes that they can be
considered
\quotation{{\em two manners of expressing the preindividual}} (7;
original emphasis). This concept of preindividuality certainly
relates to the dynamics engendered by converting
\quotation{pre-formats} into output formats, such as is done in the
construction of this thesis. When Simondon speaks of a
preindividual nature that
\quotation{is a source for future metastable states from which new individuations can emerge,}
he could easily be referring to the Markdown source format I am
using---Markdown could just easily become and OpenOffice.org
document as an HTML file (8). I will return to this when I discuss
the specificities of my workflow.

The final concept is that of transduction, the definition of which
I already presented in relation to source code. There are several
additional attributes to consider. First is its position as a
totalizing expression of ontogenesis:

\startlongquote
Transduction corresponds to this existence of relations that are born when the preindividual being individuates itself; it expresses individuation and allows it to be thought; it is therefore a notion that is both metaphysical and logical. {\em It applies to ontogenesis, and is ontogenesis itself}. (Simondon 2009: 11)
\stoplongquote

Second is the recognition that
{\em transductions are the result of incompatibilities within the preindividual state}.
These incompatibilities are not \quote{negative} in the sense of
the dialectic---rather, they are
\quotation{the other side of the richness in potential} (11).
Transduction is the mechanism through which structuration occurs
within problem domains. As Simondon has explained that transduction
{\em is} ontogenesis, it can be understood that within this
framework all becoming is the result of problems arising through
the existence of potential within metastability.

The resolution of the problem diverges from the forms of
{\em deduction}, {\em induction}, and {\em dialectic}. It differs
from {\em deduction} in that
\quotation{it extracts the resolving structure from the tensions of the domain themselves}
rather than looking outside the domain for resolution (12). Like
induction all the terms within the domain are maintained. However,
induction seeks only the commonalities between these terms.
Transduction, on the contrary, is characterized by its
{\em losslessness}:
\quotation{each of the terms of the domain can come to order itself without loss, without reduction, in the newly discovered structures}
(12). And, while it may resemble dialectic in its resolution of
opposition, there is no presupposition of a prior time because
\quotation{{\em time comes out of the preindividual just like the other dimensions according to which individuation occurs}}
(12; original emphasis).

It is this mechanism of problem-resolution that provides a means of
testing Simondon's theory against the backdrop of the computer
history that converges today to form the backdrop of generative
design. The reflexive generation of the thesis itself provides a
focused environment of becoming that likewise enables an
examination of this articulation of ontogenesis. I will begin,
however, by applying this mechanism to several of the forms of
generative design.

\section{Examples of Generative Design}

\subsection{Information Visualization}

Information visualization, or \quote{infovis}, is perhaps the
highest profile form of generative design. Infovis involves taking
raw datasets and writing code that is taylored to displaying that
data in a visually rich manner that (hopefully) facilitates the
understanding of the data. Projects such as the
{\em Washington Post's} two-year investigation
\quotation{Top-Secret America} demonstrate the effectiveness of
infovis (Priest and Arkin 2010). By articulating raw data of
Defense Department spending through generative visualization, the
viewer is able to absorb enormous data sets through interactive
visualizations of industry-agency connections and a map of
government and company work locations.

Infovis clearly emerges from the problem domain of data absorption.
It is a hybridization of previous developments in statistical
visualizations such as graphs with the interactivity and
programmability of the computer. The existing statistical
representation methods grew out of a need for displaying discrete
items---data points---in a continuous way. These techniques of
representation can be seen as a true remediation (from number to
line), driven by the call for immediacy. Infovis extends this
impulse through its hybridization of programmability and
interactivity in response to increasingly complex forms of
information.

Infovis thus emerges from a problem domain. It preserves the terms
of its preindividuated state---the remediation of data into
visually decodable information, the urge for immediacy, and the
existence of a programmable, interactive metamedium---as it emerges
into the new reconfiguration that houses its individuated
existence. The techniques of infovis are continuously evolving as
new methods are tried, where previous techniques serve as
\quotation{the principle and the model} for the techniques that are
developed next. Infovis thus seems to follow the rules of
Simondon's ontogenesis.

\subsection{Generative Computer Demos}

Though an under-studied phenomenon in the field of new media
studies, the \quotation{demoscene} extends back to the very first
personal computers. Not long after the introduction of the personal
computer was the introduction of pirated software. This software
often required \quote{cracks} in order to bypass mechanisms
developed by the software industry to protect their products.
Groups of individuals would ddedicate themselves to working around
the copy-protection schemes in a competitive manner---it mattered
to these people that they would \quotation{release} first. Demos
arose as a means of distinguishing these groups from one
another---they would play either when a user ran the crack program
to unlock the copy-protection, or alternatively they might play as
the now-unlocked program loaded.

Due to the constraints of these early computers, generative
techniques were developed that resulted in, for instance,
constantly morphing backgrounds or endlessly running
visualizations. Visual objects could be generated and manipulated
through coded algorithms rather than presented through the
size-intensive bitmap formats of early images. Over time the
demoscene became its own genre of artistic practice, having
separated from the pirated software \quotation{warez} scene quite
some time ago.

Demos arose in the problem domain of identifying which group of
software pirates had cracked an illegally obtained program. The
terms of the preindividuated space---constrained but graphically
capable computational capacity and competitive dynamics between
groups---are carried over with the individuation of the demo-making
practice. The competitive nature of the scene means that
advancements made in an individual demo inspire other groups to
\quotation{do it better,} displaying the characteristics of a
transduction wherein modulations occur at the same time as
structuration.

\subsection{Generative Efficiency}

For workflows of a certain size and complexity, generative design
can become a matter of efficiency. Florian Cramer gives the example
of Dutch designer Pietr van Blokland, house designer of Rabobank.
Faced with the task of producing brochures in 32 different
languages, van Blokland has \quotation{thrown away} Adobe and
WYSIWYG design in his studio in favor of a generative workflow
(Cramer, Mansoux, and Murtaugh 2010).

The problem domain in van Blokland's case is feasibility. The
original term---individual documents must appear in 32
languages---remains though the landscape of van Blokland's workflow
has reconfigured around generative design.

\subsection{Extrapolative Generativity}

An emergent form of generative design relies on remediating
processual grammars of generativity from the computer into the
physical world. One such example of this is the work
\quotation{Placement/Displacement} by Edo Paulus and Luna Maurer
\quotation{in which people simulate the logic of computers} (Paulus
and Maurer 2005). Utilizing the familier \quote{if/then} grammar of
procedural programming, an algorithm is constructed by which people
seat themselves according to the gender and relative heights of
their neighbors:
\quotation{IF both of your neighbors are of the same gender as you/THEN move to the closest free seat on another row/IF the person in front of you is taller than you/THEN move to the closest free seat in your current row/REPEAT.}

This movement from one medium into another demonstrates the
fluidity with which some processual grammars migrate. The audience
participating in the piece undergo a collective individuation as
they \quotation{process} the algorithm, modifying and structuring
simultaneously.

\chapter{Operating Systems as Grammars of Process}

\section{Alan Kay and a \quote{Metamedium} Vision for Personal Computing}

In his text {\em Software Takes Command}, it is Manovich's
inclination to focus on the work of Alan Kay at Xerox PARC when
discussing the development of {\em cultural software}. He notes
that there are multiple potential entrypoints for consideration:
the work of Douglas Englebart and his team, the development of the
LINC computer at the MIT's Lincoln Lab, and Donald Sutherland's
SketchPad. The development of the Xerox Dynabook, however, is
unique in multiple ways. First and foremost is the architecture of
the software: by developing and employing an object-oriented
approach to software design, users were positioned as inventors of
new media through their ability to design their own interfaces that
both enabled and spurred new modes of creation native to the
screen. These screenic modes of creation represented a new, vital
dimension to computing---the willful, {\em shaping into existence}
through design and implementation of new digital processes. Kay's
team was specifically dedicated to applying intersections between
education and computation. In the process of teaching the system to
children and adults alike, those they taught often ended up
developing their own unique applications out of the {\em objects}
that could be shared between applications as well as extended
through the inheritance model of object-oriented programming.

Cultural software---and the {\em cultural computing} which it
facilitates---is defined by Manovich as software that is
\quotation{directly used by hundreds of millions of people} and
that
\quotation{carries \quote{atoms} of culture (media and information, as well as human interactions around these media and information)}
(2008: 3). Alan Kay is a pioneer figure in the computing world, an
individual who not only theoritically formulated a vision of the
computer as a \quote{metamedium} but also did a great deal of
practical work in order to achieve this vision. Unfortunately, as
is all too common in the lives of visionaries, key elements of
Kay's ideal never breached into the mainstream even as other
aspects were appropriated and commodified wholesale by Apple and
Microsoft.

The first, and probably most important, element of Kay's original
platform that failed to transfer from his ideal \quote{Dynabook}
personal computing platform to the commercial GUI-drive operating
systems that now define the landscape of cultural
software[\letterhat{}foss\letterunderscore{}exception] is the
concept of {\em easy programmability}. To this end Kay founded the
basis of all the Xerox PARC work in personal computing on a
programming language called Smalltalk. In his text
\quotation{The Early History of Smalltalk,} he explains that the
computer science work of the 1960s began to look like
\quotation{almost a new thing} as the funding of the United States'
Advanced Research Projects Agency (ARPA) led to the development of
techniques of \quotation{human-computer symbiosis} that include
screens and pointing devices (Kay 1993). Kay's focus became
investigating what, from this standpoint of
\quotation{almost a new thing,} what the
\quotation{actual \quote{new things} might be.} Any shift from
room-size, mainframe computers to something suitable for personal
use presented new requirements of
\quotation{large scope, reduction in complexity, and end-user literacy}
that
\quotation{would require that data and control structures be done away with in favor of a more biological scheme of protected universal cells interacting only through messages that could mimic any desired behavior}
(Kay 1993).

To this end, Kay and other members of the PARC team developed this
Smalltalk language to facilitate a
\quotation{qualitative shift in belief structures---a new Kuhnian paradigm in the same spirit as the invention of the printing press}
(Kay 1993). The technical details of Smalltalk are indeed
impressive to those who can read and understand them: Smalltalk is
{\em object-oriented}, {\em dynamically typed},
\footnote{For the sake of reference, the idea of dynamic typing remains
controversial among programming language designers from the
implementation of Smalltalk in the 1970s until this very day.}
and {\em reflective} ({\em Smalltalk} 2010). All three of these
approaches have found increasing adoption in computer programming
languges. For instance, Yukhiro Matzumoto's popular language Ruby
offers all three of these characteristics. That the processual
grammar---that is, the organizing logic---of Smalltalk continues to
influence language design today is only one example of the ways in
which Kay's particular formulation of the computer metamedium
continues to hold influence today.

The Dynabook also represents a clear processual grammar, one that
defines the metamedium of computers as an assemblage of processes
that incorporates hardware, an operating system, and a capacity for
programming. While the pedagogical component of his work---that
programming should be accessible to non-professionals---is somewhat
missing from current instantiations of the Dynabook's organizing
logic is alleviated to some extent that the three major
platforms---Windows, Mac OS X, and GNU/Linux---either ship with or
provide downloads for programming tools.

The processual grammars of the graphical user interfaces developed
at Xerox PARC have likewise been hybridized and extended within the
assemblages of modern operating systems. The original individuation
of the GUI as a distinct medium has resulted in a specific
modication and structuring of the human-computer metastability. It
was not even required that the systems at Xerox PARC were
commercially successful---their individuation resulted in
transduction across the system due to the potency of the
interface's metaphors. The organizational logic of windows and
cursors has been embraced by every graphical user interface up
until the advent of small, touch-screen capable smartphones (which
use a modified version---or, perhaps, subset---of the PARC
processual grammar).

The individuation of the Dynabook retains key conditions from its
preindividuated landscape. The impulse towards immediacy is
obviously retained. Interface elements such as windows and mice had
already developed prior to the Dynabook in systems such as those
developed by Douglas Englebart or the LINC computer that emerged
from Lincoln Labs. Severo Ornstein, who worked on the LINC prior to
moving to a position at PARC, comments
\quotation{" (!CITE!). Ornstein's observation demonstrates perfectly the carry-over of terms that, when reconfigured, produce a significantly altered potential. That the processual grammar embodied by the Dynabook (and its cousin the Alto) has since moved on to}modify
and structure" the entire equilibrium of human-digital process.

\section{The Evolving Nature of Operating Systems}

The conceptions and roles of operating systems have been evolving
along with the hardware. The original operating system of the
computer could perhaps be said to be {\em human}, in that the
computer was once attended by people known as \quote{operators}
whose responsibility it was to load programs into the machine and
to maintain it in working order. These operators were an element in
an organizational grammar called {\em batch computing}. Batch
computing was driven by a logic of maximizing computer time, which
was at this point quite an expensive commodity. The operators also
served to insulate the machine from any accidents, such as a
spilled cup of coffee, that might damage the computer and result in
expensive downtime.

Batch computing had serious ramifications on {\em programmer time},
however. Batch processing involved queuing programs in a linear
order. Each program ran one at a time, and if the program
failed---as many do---the programmer would have to return in to the
back of the line once the problem had been debugged. Batch, then,
represents a particular processual grammar that elevated computer
time as a priority over human time. It is a distinct result of
particular processes, from organizational tendencies toward
hierarchies and division of labor to technological and
manufacturing constraints that resulted in a high cost for
computing time.

Inevitably, however, batch computing would face a challenging
logic. The prioritization of machine time over programmer time is
clearly pregnant with conflict. The programmer, who could possibly
know exactly what went wrong with a failing program and how to fix
it within minutes, would have to wait hours, minutes, or even days
before being able to run the modified version. If there were
another bug, the process would be repeated. This led to large
debugging cycles even for relatively minor problems (Ornstein 2002:
40--41).

The processual grammar that developed in response to this conflict
within the metastability of early computing was {\em time-sharing}.
Time sharing first began simply by allowing programmers to operate
the machine themselves and to continually modify their program and
test it during the debugging process. Initially this practice was
found only on the margins of computing, in specialized research
labs that had the resources to allow this kind of intimate
programmer-computer interaction. However, economic, technological,
and social processes continued to evolve: social adoption of
computers was increasing as more and more problem domains turned to
\quotation{computerization} in order to solve their problems,
driving economic processes to invest in the development of new and
improved technological developments that in turn drove down the
cost of new computers, lowering the barrier for adoption into new
problem domains, and so forth. This dynamic eventually led to the
development of operating systems---and programmer
interfaces---designed to facilitate the contemporaneous utilization
of a given machine by multiple individuals at the same time.

Even after this transition to hardware, the operating system of a
computer was intimately tied to its hardware. The result was a
proliferation of platforms and a staggeringly complex ecosystem of
effort duplication as computerized processes were written over and
over again in the processual grammars specific to eahc platform. It
was the development and {\em dissemination} of Unix by software
engineering luminaries Ken Thompson and Dennis Ritchie that sparked
the ontogenesis of operating systems as a
\quotation{hardware independent} process, enabling
{\em cross-platform code} as a dimension in the software process.
While operating systems had been previously developed to abstract
the system to some extent, for instance to hold segments of code
for handling routine tasks (incidentally, these segments of code
are often called \quote{code routines}) such that they need not be
input every time that operation is to be performed, the
introduction of Unix accelerated this process of abstraction.

In 1999, more than twenty years after this milestone in
abstraction, speculative fiction writer and trained programmer Neal
Stephenson released a novella-length essay entitled In the
Beginning, Was the Command Line (Stephenson 1999). This essay
delivers an analysis of (consumer) operating systems and a critique
of the reign of the graphical user interface. By means of
distinguishing the cultures and manifestations of the operating
systems---Microsoft's Windows, Apple's original Mac OS, Be's BeOS,
and GNU/Linux---Stephenson offers cars as analogy. Mac OS becomes
an expensive European car, streamlined but expensive to maintain.
Windows becomes a station wagon, the un-sexy, utility-oriented
choice of the masses. The (now-defunct) BeOS is a Batmobile. And
GNU/Linux is a tank without a price tag.

Computer programmer Garrett Birkel, writing in his update/response
to Stephenson---\quotation{The Command Line in 2004}---notes the
extreme pace of evolution of computer hardware. Despite the
expanding set of elements contained within a personal computer
\quotation{we don't call our new Dell machine a \quote{computing collective}. We consider it one device, with a singular name. And our concept of an \quote{Operating System} has evolved right along, neatly obscuring this complexity}
(Birkel 2004). This leads Birkel to clarify that what Stephenson's
text concerns itself is not so much the distinctions and
separations between hardware and software as the mode of
interactions with which we engage information:

\startlongquote
The crucial separation here is not between the computer (hardware) and the Operating System (software). Those are so deeply integrated these days that they have effectively merged into the same beast. The crucial division is between ourselves, and our information. And how that division is elucidated by the computer, with hardware and software working in tandem, really determines how useful the computer is to us. In other words, it's all about User Interface, and even though "In the Beginning, There Was the Command Line", it's also true that In The Beginning, Information Took Fewer Forms. (Birkel 2004)
\stoplongquote

The operating system is the initial site of formation for the
processes that define this division between \quotation{ourselves}
and \quotation{our information,} but it is far from the end of
story, and perhaps far from the most influential. I will return to
Stephenson and Birkel in the next chapter during the discussion of
the command line and the evolution of programmer interfaces. At
this point, there is no true means of separating hardware from
software---without an operating system, there is no means for
hardware to access much of its own functionality. The materiality,
then, of a personal computer is inherently tied to a processual
grammar that hybridizes hardware and operating system---without an
OS, the PC is not a metamedium---it is dead weight.

\subsection{Unix as a Processual Grammar}

The aforementioned capacity of Unix to migrate across hardware
while maintaining a consistent interface to both users and
programmers has been criticized as well as lauded, for example in
the {\em UNIX-HATERs Handbook} wherein the authors liken it to
\quotation{a computer virus with a user interface} (Garfinkel,
Weiss, and Strassman 1994: 4). The author's complain that the
portability of Unix results from an under-designed infrastructure,
which they call {\em incoherent}. The virus metaphor works, for
them, because of this portability, the ability to
\quotation{commandeer the resources of hosts,} and Unix's capacity
for rapid mutation (Garfinkel, Weiss, and Strassman 1994: 4).

This capacity for rapid mutation is evidence of Unix's
{\em evolutionary superiority} over other operating systems, which
the UNIX-HATERS note is not concomitant with
{\em technical superiority} and, they assert, in the case of Unix
the balance is far too the side of evolution over technical
soundness (6). Unix, then, could be considered a successful hybrid
from the standpoint of Lev Manovich's evolutionary metaphor. By
hybridizing the grammar of hardware independence with processes of
time sharing and source code distribution, Unix successfully spread
throughout the metastability of the computer industry. But these
processes that were hybridized behind the processual grammar of
Unix are not necessarily media in any traditional sense, leaving
Manovich's explanation that successful hybrids are constructed from
the \quotation{genes} of media an incomplete articulation of what
in reality unfolded.

From its initial existence as a system shared freely in code across
research institutions to its fractured existence as inconsistently
implemented proprietary offerings during the \quotation{Unix Wars}
to its resurgence as free (GNU/Linux) and open source
(Free/Open/Net/etc-BSD) software, Unix exemplifies the concept of
processual grammar. As the success of GNU and the Linux kernel
imply, it is possible to develop systems that follow this
processual grammar without incorporating any of the source code
found in Thompson and Ritchie's original release. The
standardization of a Unix specification in light of increasing
incompatibility between industry implementations demonstrates how
grammars of process facilitate and organize processes: as the
implementations diverged into incompatible versions of the
organizing logic, processes could no longer be easily shared
betweeen platforms. Only a return to a shared grammar offered
reprieve from this damaging fragmentation.

\placefigure[here]{Individuation of the Unix processual grammar over time. Image provided by WikiMedia.}{\externalfigure[images/unix_history.png]}

It is also an example of transduction in that Unix preserves the
original terms of its preindividuated state as it solves the
problem of fragmentation that resulted from a proliferation of
incompatible operating systems: the command-line interface,
time-sharing, and a desired capacity for writing
machine-independent programs. The metastability after its
individuation is irrevocably changed (much to the chagrin of the
UNIX-HATERs) as the processual grammar of Unix propagates
throughout the computer industry. When system interoperability
again became a problem due to proprietary deviations from the
original Unix grammar, a process of standardization was undertaken
in order to resolve the issue.

Another effect of the \quotation{proprietization} of Unix was the
introduction of a replacement version of Unix emerging from from
Richard Stallman's ideo-ontological premise for software.
Stallman's {\em free software}, has completely rearranged the
metastability of operating systems. Though his GNU (GNU's Not Unix)
operating system project is, as a whole, unfinished after more than
two decades of development, the environment surrounding that
effort---the GNU compilers, the libraries of code upon which they
depend, and a vast array of available commands and
applications---provides a \quotation{genetic library} from which
one can extract whatever processes are useful for one's own ends
(provided one follows the rules of the codes licensing terms, the
GNU Public License (GPL)). As a result, GNU code appears in not
only virtually every non-proprietary platform, it also ships with,
and provides important underpinnings to, that proprietary flagship
of the designing class: Mac OS X.

\subsection{Mac OS X: Process Hybridity in Action}

As the Apple platform has long been the preferred environment for
professional designers---since before OS X's first release in
2001---it invokes itself as an important object of study. The
architecture of its current operating system allows us to
investigate the concept of {\em process hybridity}. Mac OS X is a
multi-layered system that contains both open and closed source
elements with distinct historical lineages. As an assemblage, it
can be represented as containing a high degree of process hybridity
as it combines not only various separate projects into unified
commodity, it also bridges ideological boundaries by combining
proprietary, open, and free code into a single commercial object.

This hybridity reaches into the very core of OS X, into the kernel
itself. The kernel of an operating system handles the specific task
of delivering the instructions deriving from software to the
specific pieces of hardware that pertain to those intructions. A
simplistic analogy can be made to a traffic cop, whose role it is
to direct traffic in a way that the competing interests at an
intersection proceed in an orderly fashion. Kernels come in two
flavors: the {\em monolithic kernel} and the {\em microkernel}. The
distinction between the two resides in the size of the tasks they
are expected to perform, as well as a material distinction between
where operating system code should be executed ({\em kernel} memory
or {\em user} memory) ({\em Kernel (computing)} 2010).

Without entering into a lengthy discussion of the differences
between the two---a discussion that has sparked a great degree of
vitriol between their respective advocates---let us note that a
monolithic kernel contains the code for everything from networking
to video display drivers to encryption mechanisms. A microkernel,
on the other hand, delegates these processes to {\em servers} that
run outside of the kernel's memory space in the user memory (or
{\em userland}, as it is often called). The result is that there is
a great deal less chance of crashing a system that runs on a
microkernel, as a bug in user space cannot affect the operation of
the hardware whereas a bug in kernel space necessarily can.
Microkernel's are also much more modular, as servers can be
replaced with (theoretic) ease due to the fact that it sits outside
the kernel's code. OS X's kernel, however, is a hybridization of
the monolithic and microkernel designs ({\em XNU} 2010). Due to
speed concerns, it was decided that elements of the FreeBSD
project's monolithic kernel would be welded onto the Mach
microkernel.

\placefigure[here]{The architecture of Mac OS X. Note the hybridization of BSD and Mach within the kernel section of the diagram. Image courtesy of WikiMedia.}{\externalfigure[images/os_x_architecture.png]}

Moving up from the kernel we encounter the system utilities. These
are directly imported from BSD Unix, specifically FreeBSD. This is
the last layer of what constitutes the \quotation{core OS}, which
is released under an open-source license. It must be noted,
however, that the vast majority of code within this part of the
operating system was licensed under terms that allow Apple to
withhold any and all code whatsoever (i.e. {\em open source}
licenses such as BSD or MIT). The major exception to this is the
GNU C Compiler (GCC) and its attendant libraries, which are
{\em free} (as in freedom) software and is licensed under terms
that force Apple to make available the code of their modifications
whenever they publicly release new binaries of their version of the
GCC. On the other hand, specific improvements to BSD-licensed code
is available only at Apple's whim. It is impossible to monitor
whether the source code they make available represents all the
improvements they have made or whether they have been selective
with their source in order to maintain their proprietary advantage.

Above the core OS level we find further evidence of OS X's storied
history. The roots of Mac OS X reside in the operating system
NeXTSTEP, developed at Steve Jobs' NeXT, Inc. after he left Apple
in 1985. Like all GUI-based operating systems, NeXTSTEP necessarily
incorporates a large number of metaphors developed first at Xerox
PARC. In addition, however, it was built from the ground up to be
easier to develop applications for than other operating systems at
the time. To this end the Objective-C language, object-oriented and
heavily influenced by Smalltalk, was adopted. While certain systems
(such as the microkernel) might be written in C, applications were
developed in Objective-C and all the servers above the kernel spoke
exclusively with applications in this language. The programming
environment was also a leap forward in ease of use: GUI windows
could be designed in a WYSIWYG fashion, where their component
widgets (buttons, menus, etc) could be easily tied to blocks of
code which would inform the computer what was to be done upon a
given widget interaction. NeXTSTEP thus represented somewhat of a
resurgance for Alan Kay's original vision of personal computing.
However, NeXT's first computers were priced at \$9,999
dollars---resulting in slow adoption outside of academic or
institutional contexts.
\footnote{It was within such a context that Tim Berners-Lee developed the
first web browser, WorldWideWeb, on a NeXT computer.}

Three other hybridities occur within this top level of Mac OS X:
Carbon, Classic, and Java (the programming environment of NeXTSTEP
was renamed Cocoa in OS X). Carbon was developed as a new way of
writing Macintosh applications that would allow developers an
easier transition path from Mac OS Classic to OS X. Applications
written for Carbon could run on both the older Mac OS as well as OS
X, allowing companies like Adobe to refashion their code without
changing programming languages. Classic was an emulation layer that
allowed \quotation{non-Carbonized} Mac applications to run on OS X
(this hybridity has been removed since Apple's transition to Intel
microprocessors). Java is a programming language and environment
designed to allow universal execution of programs regardless of
underlying architectures: if Java has been ported to an OS, then
theoretically any Java program can then be run on that OS
\footnote{However, there are many different versions of Java, making this
ideal of \quotation{write once, run everywhere} somewhat
problematic. Microsoft was even successfully sued for trying to
hijack the language by leveraging an incompatible implementation
through the near-monopoly of its Visual Studio development
environment.}.

The hybridities present in Mac OS X have material effects from the
execution of code to the variability of its cultural enablement. It
allows for a vast assortment of applications from various lineages
and paradigms to coexist and interoperate within a single operating
system context. Short of labelling each of these hybridized
elements a \quotation{medium,} there is currently no proposed
language within new media for articulating the hybridity of the
assemblage as a whole. In this fashion, {\em process hybridity}
provides a mechanism with which we can describe the facets of OS X.

Mac OS X hybridizes not only material processes (code, hardware)
but also ideological processes (licensing terms). It's success is a
function of economic (the backing of a large firm) as well as
cultural processes (Apple's ability to mass-produce computers is a
function of their position as the introducers of the first
commercially successful personal computer). This allows OS X its
status as the only successful, commercial UNIX on the desktop as
well as its status as an easy-to-use, GUI-driven development
platform. This development platform is based on Objective-C, a
language which hybridizes the grammars of C and Smalltalk. Thus the
hybridization of process that constitutes Mac OS X incorporates
even more closely the processual grammar of Alan Kay's Dynabook.

Furthermore, OS X provides an interesting observation of the
dynamics of one of the key elements of Marshall McLuhan's Tetrad:
when the medium of the GUI, exemplified by the original,
\quotation{no command-line} classical Macintosh operating system,
is pushed to its limits, it apparently reverts---in this case at
least---to hosting a command-line interface. The reverse case seems
to be true for the CLI, which when pushed to its most extreme
becomes host to graphical user interfaces that in turn embed
command-line interfaces through programs known as
\quote{terminal emulators.}

\section{Individual Operating Systems Are Processual Grammars}

A media theory based on either specificities or effects would
likely characterize individual operating systems as separate media.
Certainly there is great material variance (and thus variance of
effect) from a user interaction point of view between various
operating systems. However, a close examination of these
specificities implies that beneath the variation in interface lie
common processes---it is the logic of their organization that
varies. File manipulations, drawing to the screen, and the
multitasking of multiple applications at once (to name but a few)
are processes common to all modern operating systems. While
implementation and organization differ between individual systems,
from a process-oriented perspective there is only one medium
involved: the medium of the operating system. In other words,
grammatic specificities do not imply the emergence of a new medium.
To argue otherwise leads us to the trap of labelling each iteration
of software that changes its interface a separate medium.
{\em It is not the surface of a system that determines its \quote{medium-ness}}.

\chapter{Text as Interface/Text as Process}

The command-line interface (CLI), once a culturally universal site
of intersection between human and digital process, has found itself
virtually superceded by the visually metaphoric instrumentation of
the graphical user interface (GUI). The mechanism of this
transition from CLI to GUI within mainstream computing was the
introduction of Microsoft Windows into the ecosystem of
IBM-compatible PCs. The result was the injection of an additional
semiotic layer, charged with a new modality of visual
signification, between the user and the hardware (Stephenson 1999).
For almost two decades consumer versions of Windows, however, were
\quotation{DOS front-ends} that could not function without real,
historical dependencies fulfilled by the presence of DOS deep
within the guts of the operating system. Windows 1.0, for instance,
used DOS's file operation functions ({\em Windows 1.0} 2010). This
dependency on DOS recedes over time, eventually disappearing
entirely in Windows XP, in which the DOS interface and
functionality still exists but has migrated out of the substrate
and into a virtual machine ({\em Windows XP} 2010).

The roots of the command line lie in a very physical process: the
teletype. A teletype resembles a typewriter in that it presented
the users with a standard typewriter keyboard as a control.
Pressing a key would result in an inked stamp of that keys
respective character smacking onto the paper and retract, leaving
its mark. Simultaneously the triggering of the key might be punched
into a tape as a binary sequence representing the character. If so,
the control was thus separated intrinsically between human and
digital---it was not, as in todays keyboards, simply electrical
signals converted into numbers transparently beneath our fingertips
but rather also a physical instantiation of the sequence on a paper
strip. The screen of this human-digital intersection was
instantiated on the same paper as the recording of the input, using
the same ink and stamps but now powered by the response of the
machine to its human input.

Stephenson identifies an extremely formal dynamic of interacting
through teletypic screens he encountered when learning to program
in high school:

\startlongquote
Anyway, it will have been obvious that my interaction with the computer was of an extremely formal nature, being sharply divided up into different phases, viz.: (1) sitting at home with paper and pencil, miles and miles from any computer, I would think very, very hard about what I wanted the computer to do, and translate my intentions into a computer language--a series of alphanumeric symbols on a page. (2) I would carry this across a sort of informational cordon sanitaire (three miles of snowdrifts) to school and type those letters into a machine--not a computer--which would convert the symbols into binary numbers and record them visibly on a tape. (3) Then, through the rubber-cup modem, I would cause those numbers to be sent to the university mainframe, which would (4) do arithmetic on them and send different numbers back to the teletype. (5) The teletype would convert these numbers back into letters and hammer them out on a page and (6) I, watching, would construe the letters as meaningful symbols. (Stephenson 1999)
\stoplongquote

In this text, titled
\quotation{In the beginning was the command line,} Neal Stephenson
proceeds to identify the underlying mechanisms of human-digital
processual intersections:
\quotation{computers do arithmetic on bits of information. Humans construe the bits as meaningful symbols.}
He notes, however, that this act of translation is increasingly
obscured by ever-increasing metaphoric abstraction, starting with
the GUI and carrying on over the course of the evolution of
graphical interfaces. Command-line interfaces are close to the
bottom of the \quotation{stack} of the cross-translation between
symbols and bits, whereas
\quotation{[w]hen we use most modern operating systems, though, our interaction with the machine is heavily mediated. Everything we do is interpreted and translated time and again as it works its way down through all of the metaphors and abstractions}
(Stephenson 1999).

Text, as the least abstract of the available sites of symbol
translation between digital binary forms (which can be considered
the text of a different alphabet) and human process, is the formal
level of computing. As such, the general non-consideration of the
command-line interface in new media discourse is a disservice to
the metamedium with which much of our discourse concerns itself.

Florian Cramer has identified an instrinsic
\quote{contradictory nature} of the computer (Cramer 2005). This
contradictory nature envelops the command line as well. A tool at
once more powerful and more flexible yet equally more opaque and
unyielding. To begin to understand the command-line is to begin
shooting lit arrows in the dark, lighting fires of process that can
burn the results of their functioning onto your harddrive, your
graphics card, your BIOS, or your network as easily as onto your
screen. The Unix command

\type{rm -rf /*}

will erase the entire contents of a Unix filesystem from the hard
drive. The code for \type{rm} loaded into memory survives to delete
itself from a core component of its materiality, that is, the raw
1s and 0s on the magnetic platters that constitute the persistent
body of the command. It will not, however, survive the reboot
inevitably awaiting such a mangled system.

\section{Remediation and the Command Line}

Today's command-line is driven by computer keyboards and its
outputs are displayed on computer screens, often within the context
of \quote{terminal emulators} running in a GUI environment. This
very idea of \quotation{terminal emulation,} however, leads
directly into the specific history of human-digital interfaces.
Terminals were the previously dominant form of interface, arising
after the evolution of time-sharing systems. Though they were
superceded with the advent of microcomputer workstations, their
legacy remains with us today. An inquiry into the history of
terminals, and their predecessors, allows a glimpse into the role
{\em inertia} plays in the course of computer evolution.

\subsection{Interface: From Wires to Text}

Long before the interactive, real-time interfaces of today's
computers and yesterday's terminals, computers were programmed
exclusively through the use of wires to direct the sequence of
operations that defined a computation (Ornstein 2002: 10). Severo
Ornstein, a programmer from what he calls
\quotation{the Middle Ages,} writes in his account of computer
evolution that as a child his wife, whose father was worked in the
ENIAC
\footnote{The ENIAC was one of the very first programmable computers. It took
up an entire floor of the building it was housed in.}
project that, was deemed one of the first
\quotation{un-programmers}---her job was to remove wires and sort
them by length so that they could be used again (11).
\quotation{Spaghetti code,} a term still in use today to describe
convulated source code, originates in this early, extremely
material form of programming.

A separate form of programming had already been in use in analog
machines such as industrial looms and tabulating machines: the
punch card. Eventually computers were taught to read instructions
from these cards, and it is the punch card that typifies the age of
batch computing. Punch cards were volatile in that the misplacement
of a single card would render the program unexecutable, as would an
errant punched hole or a dog-eared card. Punch cards were also
complex in their instantiation---programmers did not punch the
cards themselves but instead handed their machine instructions to
keypunchers who proceeded to translate these instructions onto the
punch cards. According to Ornstein's experience, these keypunchers
were exclusively female as well as generally underpaid (Ornstein
2002: 38--39; Chun 2004).

To be quite clear, I view punch cards as a distinct medium. They
represent a clearly reflexive site of mediation that enfolded
multiple processual grammars. In fact, punch cards were in use
decades before programmable computers were first developed---their
use in programming, then, represents a distinct organizational
logic which changed over time. Within their specific use in
computers, punch cards are quite interesting in that they handled
multiple processes that have since become handled by discrete,
specialized processes. According to Douglas Crockford, punch cards
served as {\em memory}, {\em storage}, {\em archive},
{\em network}, and {\em user interface} (2010a). The
\quotation{outsourcing} of these functions into discrete processes
represents an opportunity to question remediation. Were punch cards
remediated into RAM (memory), magneto-optical drives (storage),
magnetic and optical backup options (archive), and Ethernet
(network)? And what of user interfaces? Do they remediate punch
cards?

The pull towards immediacy seems to be at play in that all of these
replacement processes offer distinct improvements in user
experience. However, each of these developments can also be
explained as specific process hybridities that evolved under new
grammars that do not embed and extend the punch card. Rather they
rise to the challenge of providing functionality that once relied
solely on the processual grammar of punch cards. Of all the
material specificities of those cards, the only remaining evidence
of their existence in the replacement grammars is an 80-character
limit that is still observed in the coding guidlines of many open
source projects. However, observance of this limit is today
attributed either to readability or to compliance with those who
may still be working with devices such as printers and terminals
that were developed when computers enforced this limit for
compatibility reasons (Slashdot 2007). The organizing logic of the
punch card has completely faded into history, the underpaid
keypunchers and insulating layer of operators largely forgotten.
Perhaps the answer lies in the command-line interface that
represents the next site of human-digital interaction after the
obsolescence of batch computing.

\subsection{The Command Line is a Medium}

The processes embodied in file operation commands instantiate into
material effects on hard drives. They are abstractions of
processual hybridization that results in the same command in the
same operating system having the same effect on the file system.
The modules loaded into the assemblage offering this abstraction
depend on the format of the file system (NTFS, HFS+, ext*, etc.),
the motherboard-to-disk controller protocol (IDE, SCSI, SATA, etc.)
and the driver specificities of that disk controller. All of these
elements are unique, digital assemblages. The
{\em embodied processes} that are typed commands cannot be
accurately held to the standard of a theory that is based on a
conception of media as containing and extending previous media. It
is not that the medium of punch cards that is
remediated---{\em rather the command-line assembles processes that previously required punch cards and organizes them into a new logic through a specific grammar}.

These commands, these arrows flaming into the dark, have only the
output of text in order to satisfy the needs of {\em immediacy},
but when teletypes (and later terminals) first arrived this level
of interactivity was unprecedented. This interactivity inspired the
development of many new media. Text editing and email are two
commonly hybridized processes that have been around since the early
beginnings of time-sharing, and Douglas Crockford is quick to note
that social networking tools and blogs appeared in early
time-sharing systems as well. However, due to the natural reliance
of the keyboard as the control of the command line, these
interfaces are often rely on combinations (or \quotation{chords})
of key presses for purposes of navigation and process
instantiation. This is a new type of immediacy only became
available through the development of interactive teletypes, which
took the medium of the typewriter and developed a new
organizational logic that hybridized processes of modems and tape
manipulation into a new assemblage. This chord-based immediacy
comes with a steep learning curve but, once mastered, it rewards
the user with productive potentials beyond what is accomplishable
through the generally-considered-superior GUI. Some hackers joke
that their favorite operating system is their text editor
\type{emacs}, which is noted for accomplishing everything from
coding to typesetting (through the powerful AUCTeX extension) to
email and calenders to web browsing. All interaction is
accomplished through these chords of key presses.

The command-line is a medium because it reflexively develops new
grammars of process. Beginning with the re-purposing of the
typewriter into the teletype, the command-line then moves to the
terminal. After the introduction of the GUI, the command-line moves
comfortably into terminal emulation programs. Each of these
transitions is characterized by an increase in immediacy,
demonstrating that at least this driving force of remediation is at
play. However, despite changes in the materiality of the
{\em interface} in front of the human, the site of action of the
command-line remains the computer. The materiality of interfaces
even constrained the capacity of the command-line, as in the case
of the 80-character line width limit found in terminals such as the
DEC VT100. However, as processes evolved, the command-line medium
moved from one organizational logic to another. Rather than an
example of remediation, this travelling of the command-line
represents a medium that continuously developing new processual
grammars by hybridizing new processes according to its own
organizational logic---and with an apparent drive for immediacy.
The quest for immediacy should not be seen as proof of remediation,
but rather as a seemingly ever-present drive stemming from
somewhere within human process.
\footnote{Considering the traditional programmer obsession with
\quote{efficiency} in relation to immediacy may be a fruitful
endeavor in questioning the impulse of these changes, however.}

\subsection{Commands are Processual Grammars}

The grammar of Unix includes the concept of \quote{pipes}, which
are used to redirect the output of one command into the input of
another. There are three predefined pipes: \type{STDIN},
\type{STDOUT}, and \type{STDERR}. These are mapped to the keyboard,
the screen, and a background program called the \quote{syslog,}
respectively. Like much of Unix, however, these standard pipes can
be reconfigured in order to point elsewhere. Indeed, this is one of
the most powerful aspects of the CLI. By redirecting the STDOUT of
one program into the STDIN of another, a sort of \quote{freestyle}
hybridization of process can be achieved.

For instance, running the following command

\type{ps -aux}

results in a list of all running processes within the system being
printed to the screen. Since the screen is simply the place where
\type{STDOUT} is pointed, however, one can pipe the output of that
command into the input of another. The following

\type{ps -aux | grep -i ps}

displays only the processes belonging to the program \type{ps}. If
those results are better served by saving in a file, then

\type{ps -aux | grep -i ps | cat > ps.output}

stores the results in a file called \type{ps.output}.

This displays the central problem with Manovich's media hybridity.
Clearly there is a hybridization occuring: the results of each
sequence of commands vary in effect. Described as a case of media
hybridity, however, then each individual command
\quotation{becomes,} discursively, a medium. Rather than neatly
packaging these operations into unitive
\quotation{new media species}, the lens of transduction allows one
to more rigorously consider the specific arrangements of the
organization of these command sequences and, it follows, software
in general.

\chapter{Top Down/Bottom Up}

This chapter concerns the conditions specific to the workflow of
generative typesetting involved in this thesis. This includes both
a historical and compositional survey of the file formats involved
as well as a discussion of the specificities involved in the
organizational logic of this workflow. To begin I will make a
distinction between two modes of typesettting found within the
computer along a division of \quotation{top down/bottom up}
approaches that will encompass this historical-compositional
survey.

\section{WYSIWYG}

WYSIWYG, meaning \quotation{What You See Is What You Get,} is a
mode of interface design in which operations are performed in an
extremely top-down manner. In terms of typesetting, the definitive
example of WYSIWYG is Microsoft Word. Word is often deemed a
remediation of the typewriter. Indeed, it embeds and extends many
of the material specificities of the typewriter, from the line and
margin width sliders at the top to the per-line linebreaking found
within that medium.

In this sense, MS Word is clearly an instance of attempted
{\em immediacy}. By remediating the then-familiar modality of the
typewriter into the context of the computer screen, Word created a
state immediacy that was instantaneously seen as revolutionizing .
But if the typewriter is a medium, something which few media
theorists would likely argue against, then is Microsoft Word a
medium as well? Here again we see the slippery slope of media
conflation within the metamedium. The boundaries must exist
somewhere, if only for the purposes of coherent discourse. Just as
one is unlikely to consider different models and brands of
typewriters as separate media, it similarly makes sense to say that
word processing applications {\em as a whole} constitute a medium.
Variations between these applications are, once again, seen as
variations in organizational logic.

There is a distinction, however, between word processing
applications and simple, plain-text editors. While word processors
do remediate typewriters in a classical example of Bolter and
Grusin's theory, text editing applications do not remediate their
punch card ancestors. Instead they constitute a new process
hybridity, one which does not share any process that was specific
to the typewriter other than using keys to output characters. While
it is true that \quotation{what you see is what you get,} text
editors are not generally considered WYSIWYG because they lack
formatting options beyond what is available on the keyboard. These
formatting options offer space for new processual grammars to
evolve within the domain of plain text editors, as will be seen
later in this chapter.

The WYSIWYG interface is clearly a top down approach, as all
manipulations come from invoking processes onto what has already
been placed into the interface. Critical designer Femker Snelting,
member of Open Source Publishers, characterizes WYSIWYG as
\quotation{What you see is what it is,} a phrasing that emphasizes
the linear workflow imposed by the interface (Snelting and
Huyghebaert 2010). This linear workflow is fully geared towards the
human, even to the extent of presenting users with the comfortably
remediated interface of the typewriter. From this perspective,
WYSIWYG is organized towards outputting a {\em product} via a
processual grammar that privileges the human user from intent to
interface.

\section{Processed Text}

Processed text comes in two flavors: {\em semantic} and
{\em formal}. Semantic formats such as HTML and XML are far more
widely used than formal formats such as {\TEX} (and La{\TEX}/Con{\TEX}t).
While both are bottom up in contrast to WYSIWYG, there is a
distinction even here between top down and bottom up. They are both
bottom up in that the typesetting goals of any block of text are
specified at the beginning of that block, i.e.~\type{<em>} and
\type{\quote} are both typed before the block of text begins.

HTML is top down, however, because that is its rendering model. By
imbuing blocks of text with semantic qualities, one abstracts away
the process of displaying those semantic blocks. Order is imposed
from above, both through Cascading Style Sheets (CSS) and through
the rendering algorithm of a given browser's implementation. This
is a sort of outsourcing of the specifics of typesetting, a
demanding craft that---as I have learned---requires careful
attention to detail in order to produce outputs of high quality.

\subsection{HTML, or Semantic Markup is Literally an Organizational Grammar}

The concept of semantic markup predates HTML by a considerable
margin. However, HTML represents the \quote{watershed} format
through which semantic markup entered mass usage across the world.
The idea of a semantic markup revolves around a desire to separate
content from presentation. The effects of such a separation include
machine-readability: even though a computer might not have any
sense of what a \type{<title>} is, it can still tell you exactly
text is being marked with such a tag. This machine readability is
the primary problem domain from which semantic markup evolved.

It is the dream of the World Wide Web Consortium (W3C), headed by
HTML inventor Tim Berners-Lee, to create a
\quotation{common information space} based on principles of
\quotation{universality} (Berners-Lee 1998). This universality is
heavily based on the semantic capacity of HTML. To further this
same goal, the W3C has also sought fit to create a format, called
eXtensible Markup Language (XML), in which a user can define their
own semantics so long as they follow XML's syntax. The success of
this approach is evident in the widespread adoption of XML as a
basis for formats as diverse as OpenOffice.org documents and the
RSS feeds driving much of the \quote{mashability} of the current
Web.

HTML evolved from the context of Standardized Generalized Markup
Language (SGML), which in turn was developed out of the context of
the Generalized Markup Language (Crockford 2010b). Thus, the roots
of HTML extend back into the 1960s, when GML was first codified at
IBM. It was specifically designed for machine-readability and it
was put into use by organizations that required documents to be
readable over long stretches of time.

From a process-oriented perspecive, HTML is not a medium. It is a
process hybridity assembled from existing developments in semantic
markup and hypertext. This is perhaps a counter-intuitive statement
considering the direction of existing media discourse. However,
just as with our example of makes and models of typewriters,
different organizational logics of hypertext are not sufficient to
attain the label of medium---the medium remains hypertext.

\subsubsection{HTML as a Productive Site of Transduction}

From the beginning of the World Wide Web's popularity it has been
the site of constant transduction as the {\em potentials} of a
\quotation{common information space} inspired solutions to problems
with the original implementation. First came the introduction of
the \type{<img>} tag by in the University of Illinois' Mosaic, a
modification to HTML that Berners-Lee originally opposed (Crockford
2010b). This satisfied an urge for multimedia by hybridizing the
grammar of HTML with existing digital image formats. Along with the
\type{<img>} tag came a host of others, such as \type{<blink>} and
other means of modifying the presentation of a page.

In response to this proliferation of presentation-oriented tags
(which they had never approved), Berners-Lee and his team at the
W3c introduced Cascading Style Sheets (CSS). CSS provided a means
for informing the browser of the intentions the page author had for
the presentation of her HTML. This further developed the ideology
of \quotation{separating} form from content and forever changed the
specificities and experience of the Web.

The introduction of JavaScript and the Document Object Model
likewise engendered significant changes in the ways that user's
experience HTML. When rendered in a modern browser, HTML becomes
mapped to the DOM in a hierarchical system of parents, children,
and siblings. JavaScript provides a means for interacting with the
DOM in real-time, as the user is browsing the page. Since the DOM
also tracks the CSS associated with HTML elements, and CSS
(theoretically) controls the presentation of those elements,
JavaScript can modify the DOM so that certain pieces of the page
appear or disappear in response to arbitrarily defined actions.
This is the mechanism that the HTML version of the thesis uses in
order to display only one chapter at a time---the entire thesis is
present in the HTML of the page, but only select parts are set to
\type{visible} by JavaScript at any given time. The development of
AJAX in the first decade of the twenty-first century further
changed the user's experience of the Web by enabling the insertion
of new HTML content from the server without loading a revised
version of the page in the user's browser.

Thus, even from a specificity or effect standpoint, HTML is not a
medium: HTML in today's Web is irrevocably altered by the
integrality of JavaScript, the DOM, CSS, and (increasingly) AJAX to
it's modern presentation and experience. Removing these elements
destroys the specificities and effects that currently characterize
the Web, because the Web has become a process hybridity that relies
on much more than HTML alone. This fact also belies the idea that
form and content are separable (Schuller 2008).

In the words of Douglas Crockford, the success of HTML is
\quotation{due primarily to a lot of very clever people who found ways to make it work in spite of its inherent, intentional limitations}
(2010b).

\subsection{{\TEX}, or Text That Typesets Itself}

Donald Knuth is a prominent figure in the field of computer science
due to his series of books, {\em The Art of Computer Programming}.
The series is ongoing, but the first volumes were published in the
1960s. During the 1970s many publishing house switched to new
photo-optical systems for their typesetting. When Knuth viewed the
galley copies for a new edition of the second volume that was to be
printed using one of these new machines, he noticed that the
typographic quality had dropped precipitously (Knuth 1999: 5).
Knuth's response was {\TEX}, a programming language built specifically
for the purposes of typesetting.

Femke Snelting identifies in Knuth's writing an impulse to design
not simply {\em a} system for typesetting, but in fact
{\em the best} system (Snelting and Huyghebaert 2010). Evidence of
this appears towards the beginning of his 90-page chapter
describing the details of {\TEX}'s line-breaking algorithm. From his
perspective as a computer scientist (or, perhaps more accurate, a
\quotation{computer artist}),

\startlongquote
A properly programmed computer should, in fact, be able to solve the line-breaking problem better than a skilled typesetter could do by hand in a reasonable amount of time---unless we give this person the liberty to change the wording in order to obtain a better fit. (Knuth 1999: 69)
\stoplongquote

Line-breaking, or {\em justification}, is not only a deeply complex
problem---it is a serious issue for readability. In the words of
designer Robert Bringhurst,
\quotation{A typewriter (or a computer-drive printer of the same quality) that justifies its lines in imitation of typesetting is a presumptious, uneducated machine, mimicking the outward form instead of the inner truth of typography}
(Bringhurst 2008: 28). {\TEX}, then, is an attempt to express this
\quote{inner truth} by integrating the design principles set forth
by defining modernist designer Jan Tsischold at a level of
\quotation{mathematical precision} (Snelting 2009: 324).

In stark constrast to HTML, the {\TEX} programming language contains
no sense of semantics. Instead of \quote{tags} which notify a
machinic interpreter of the specific structural quality pf a given
piece of text, {\TEX} wraps text in commands when it wishes to process
that text in some way. Once the processing has occured and the text
has been set according to the execution of those commands, there is
no sense within the document format itself of the significance of
any given text.
\footnote{This may begin to change as the newest {\TEX}
engine---LuaTeX---supports the PDF/X standard, which allows for the
tagging of text within a PDF file.}
This processing of {\em commands} rather than {\em semantics} means
that {\TEX} is a {\em formal markup}: the markup, embedded within and
inextricably from the content, literally determines the final shape
and structure of the output.

As a programming language, {\TEX} does have the capacity to organize
commands into \quote{macros.} This capacity for organizing commands
allows {\TEX} users to avoid repeatedly typing in any patterns of
commands they use on a regular basis. Multiple
\quote{macro packages} were developed, some of them commercial and
others freely shared. The most popular of these is \LATEX. Perhaps
unsurprisingly, \LATEX was designed in order to give the creation
of {\TEX}-based documents a more semantic structure. In doing so,
however, it tends to limit the typographic choices available to the
user. This is not generally a problem within the most common use
cases of \LATEX: academic journals, especially within fields which
use a great deal of mathematical formulas. In this context,
typographic variability is irrelevant as accomplishing one's
typographic goals is simply a manner of following the semantics of
the journal's \LATEX template.

When the demands are more complex, however, this style of
insulating the user from their capacity to shape their own
typesetting process can be a total deal breaker. This was the
experience of Open Source Publishers, whose design goals could not
be reached do to various incompatibilities between \LATEX
extensions which chose to hardcode values for the user for the sake
of convenience only to end up conflicting with other extensions
(Snelting 2009; Snelting and Huyghebaert 2010). The solution they
chose was a macro package I had selected to learn when I first
discovered the relative non-configurability of \LATEX: {\CONTEXT}.

\subsubsection{{\CONTEXT} is Perpetually Becoming}

{\CONTEXT} was developed to suit the needs of the Dutch company Pragma
ADE, a company which deals primarily with the typesetting of
educational textbooks. In a \quotation{complementary approach}
\LATEX, {\CONTEXT} provides
\quotation{structured interfaces for handling typography, including extensive support for colors, backgrounds, hyperlinks, presentations, figure-text integration, and conditional compilation}
({\em contextgarden} 2010). The project traditionally had no
version numbers. Advancements were simply introduced into the code
and distributed as they occurred.

Starting in 2005, however, {\CONTEXT} was broken into two versions.
The new version, dubbed MkIV, would migrate to a new {\TEX} engine
callled LuaTeX. LuaTeX is a very interesting example of process
hybridity. Its individuation came in response to limitations in the
{\TEX} implementations existing at that time which lacked
bidirectional, multi-lingual typesetting and support for the new
font format OpenType. It significantly changes the potentials of
generative typesetting by integrating the modern programming
language Lua into the existing pdfTeX engine. In so doing, it
exposes the internals of {\TEX}'s typesetting functionality to
re-configuration in ways that simply are not possible with the {\TEX}
programming language alone. As just one example, it is
theoretically possible to do the entire generative typesetting
exercise completely within LuaTeX such that LuaTeX would take care
of generating both a PDF and an HTML file from the Markdown file in
which this thesis is actually written. This would be a
significantly different case than the way the thesis is generated
now. The field of potentiality is even much greater than that,
however, as the entirety of existing Lua code---and anything to be
written in Lua in the future---can now be used in the typesetting
of {\TEX} documents.

\chapter{Generative Typesetting}

\section{Environment of Operation}

This text is not typed in the manner that you see it. The above
section header is instead written like this:

\starttyping
  # Environment of Operation #
\stoptyping

Through the wrapper program \type{pandoc}, this input, written in a
format called Markdown, is converted into HTML and {\CONTEXT} outputs.

\startdescr{HTML}
\type{<h1>Environment of Operation</h1>}
\stopdescr

\startdescr{{\CONTEXT}}
\type{\section{Environment of Operation}}
\stopdescr

The syntax of HTML represents a semantic operation:
\quotation{Dear Mr.~Browser, treat this as a header of level 1. Treat it first according to any rules defined within CSS and, failing that, display it the way you usually do.}
The syntax of {\CONTEXT}, however, represents a macro command within a
programming language. What it says is
\quotation{call the sections of code that translate the text within the brackets to the parameters specified for the \type{\section{}} command.}

The literal \quote{writing space} of this thesis is a program
called Textroom. Textroom is a minimalist text editor in which
there are no buttons, taskbars, or other clutter. Only you, your
words, and (optionally) informational text reporting the time, word
count, percentage to accomplishing your writing goal, etc. But this
is not a new medium! It is simply an individuation directed by the
problem of distraction within the GUI and the (lack of) aesthetics
in most available text editors. It reconfigures existing terms into
a new process hybridity driven by its own organizational logic. But
it is not a new medium.

\placefigure[here]{The Textroom editor running in full-screen mode.}{\externalfigure[images/textroom.png]}

By writing in plain-text, I open myself to the opportunities
afforded me by version control systems. Developed to enable
collaboration of programmers on a code base, version control
systems can track changes in text across time (useful for this
project) and allow for massively distributed workflows involving
tens of thousands of individuals (useful for the Linux kernel).
This capacity for version control is a distinct advantage of plain
text over the more complex, binary file formats used by word
processors. Since the plain-text is organized according to the
logic of Markdown, it can potentially become anything.

\subsection{Markdown and Preindividuality}

\placefigure[here]{The file \quote{bibliography.md} is automatically rendered through the web interface of github.com, where it is stored in a version control repository.}{\externalfigure[images/github-bib.png]}

The nature of a pre-format's materiality is even more convulated
than that of the average digital object. Figure 4 displays the
unintentional rendering of the my bibliography by the github's web
site when using it to access the copy of \quote{bibliography.md}
stored there. This demonstrates the relative ubiquity of Markdown's
processual grammar.

\quote{bibliography.md} is clearly \quotation{more than unity} in
that it is a site of constant translation into other formats. Even
when it is innocently uploaded to a version control repository for
safe-keeping it cannot escape its fate as a
\quotation{more than identity.} It is an object intended for
translation, structured around the idea of becoming. In that sense,
then, it might be considered as a {\em transduction in progress},
an individuation that never ends because the potential for
translation is always there. A Markdown file's level of potential
actually increases over time as more processual grammars hybridize
Markdown into their assemblages and tools enable translation to new
outputs. This is similar to a highly charged metastability in that
it is constantly provoking new transdusctions.

Simondon states that a preindividual nature
\quotation{remains linked to the individual,} existing as
\quotation{a source for future metastable states from which new individuations can emerge}
(2002: 8). Perhaps, then, it can be said that a Markdown file
retains a great deal of its \quotation{preindividual nature,}
carrying some of the malleability of the metamedium from which it
itself individuated.

\section{Constraints}

When I first began this project and explained it to Florian Cramer,
he warned me that what I was searching for was a \quote{Holy Grail}
that had been sought by many. This Holy Grail is a
\quote{universal document format,} a mechanism that can arrange
itself to the specificities of any format that might be desired.
Many have tried, and none have succeeded. The systems become
weighted down in wrappers such as Pandoc, which require wrappers on
top of them in order to satisfy the edge cases that inevitably
result from the specificities of various formats. Though I was
skeptical that such \quotation{double-wrapping} was inevitable, my
experience has born out the accuracy of his observation.

An unfortunate constraint is the inability to take advantage of
elements of the {\TEX} landscape that are reknowned for making life
easier. The chief among these is BibTeX, which allows for a
bibliography to be dynamically generated and citations to be
inserted according to a variety of formats (that one can change
with a single line of text, if desired). By abstracting myself from
{\TEX} by using Markdown as the \quotation{pre-format,} I've lost the
opportunity to easily manage bibliographic data and instead must
input it by hand. That said, the MLA format is not currently
available in BibTeX meaning that---even if I could use this
software---the output would be necessarily shaped by the
constraints of the tools.

This tendency to be forced to simplify (or perhaps even
\quotation{weaken}) the document as a whole was raised by Femke
Snelting during a personal interview I conducted with her and
fellow Open Source Publisher Pierre Huyghebaert (Snelting and
Huyghebaert 2010). Snelting is intensely interested by the ways
that our tools shape what we make, and this is a clear example of
this dynamic.

\subsection{A Pre-Format Necessarily Complicates While it Simplifies}

Using Markdown as a pre-format complicates several issues with
typesetting documents in both HTML and {\CONTEXT}. For instance,
{\CONTEXT} includes a \type{\chapter{}} macro, which influences the
numbering of sections so that sections are relative to the chapter
number, rather than to the entire text. Thus the first section of
chapter two will be rendered into text as \quote{2.1}. The issue
arises because HTML has no similar distinction: sections are
related to the depth of the header, such that H1 is the highest
level section (the equivalent of {\CONTEXT}'s \type{\chapter{}}). In
markdown, the top-level section appears as in the example above,
that is using a single \quote{\type{#}}. My initial solution was to
include two sections with only one \quote{\type{#}} per chapter,
the first of which I would manually change from \type{\section{}}
to \type{\chapter{}}, with the result that the sectioning of the
chapter fits what is expected---chapter 2, section 1 is numbered as
\quote{2.1}.

This does not satisfy HTML, however, as there is nothing to change
the first single \quote{\#} into: it is already inserting the
highest level section, \quote{H1}. Thus, where {\CONTEXT} begins
numbering the sections within the chapter (2.1, 2.2, etc.), HTML
increments the top-level section number, so that the second single
\quote{\#} increments to \quote{3}, instead of to \quote{2.1}. If a
double \quote{\#\#} is used, the sectioning will appear as we
desire in the HTML, but will appear as \quote{2.0.1} in {\CONTEXT}.
This incompatibility requires either modifying Pandoc through its
native scripting facilities or else the creation of a specific
\quote{helper} script to create a sectioning parity between the two
output formats. As the native scripting facilities require working
in Haskell---a language I do not know---I've opted for the second
solution.

Since Markdown allows passing {\TEX} commands, I solved the problem by
using \type{\chapter{}} to designate the chapter title. Then, using
a command-line script written in the Ruby programming language, a
copy of the markdown file is created in which \type{\chapter{}} is
replaced by a single \quote{\#} and all subsequent
\quote{\#'s are increased by one}\#' until the next
\type{\chapter{}} is reached. From this copy we generate the HTML
version, while the original can be processed into {\TEX}. Perhaps this
technical description appears to be more of a computer science
discussion than it is a media theory one. However it highlights the
ways in which processes hybridize: the conflicting grammars of
{\CONTEXT} and HTML create a complication which the wrapper program
Pandoc either does not or can not address. To work around this
issue, a separate program (the Ruby interpeter) is used to
integrate a script file which deals with the problem. This is a
common feature of a command-line based workflow: \quotation{glue}
scripts are written in order to fuse processes together. The effect
is \quote{wrapping a wrapper} and from my experience there is
always a secret fear that the glue code will fail at exactly that
moment when the deadline strikes.

\section{Regular Expressions Build This Thesis}

Regular expressions represent another avenue for demonstrating
process hybridity. Since their introduction into the early text
editors \type{QED} and \type{ed} by Ken Thompson, regular
expressions have since been incorporated into many Unix commands
such as \type{grep} and \type{awk}, newer editors such as \type{vi}
and \type{emacs}, and programming languages such as Perl, PHP,
Python, Ruby, and many more ({\em Regular expression} 2010).
Regular expressions are a means for describing parameters of text
searches, whereby arranging esoteric control characters in and
around a segment of text allows for finely tuned pattern matching
within text.

The processual grammar of regular expressions has been adopted by
nearly every major programming language: from Wikipedia, the list
includes
\quotation{Java, JavaScript, PCRE, Python, Ruby, Microsoft's .Net Framework, and the W3C's XML Schema}
({\em Regular expression} 2010). This list of languages refers to
those who have hybridized some form or derivative of Perl's
implementation of regular expressions, which is considered more
robust than Ken Thompson's original version.

Is it possible to say that these programming languages are
\quote{remediating} the regular expressions from Perl? It is not
beyond reason to assert that programming languages are
\quote{mediums}---Ken Thompson has referred to Smalltalk as a new
medium, for instance (!CITE!). However, mapping the term medium
onto a programming language falls into the same trap of stretching
the term medium until it becomes incomprehensible. Do different
versions of the same language, representing different capabilities
and even incompatible syntax changes, constitute separate mediums?
What is useful about applying the term medium here, other than it
enables us to discuss the prolific implementation of regular
expressions as an example of \quote{remediation}?

Programming languages often borrow concepts from each other, as
this example of regular expressions clearly demonstrates. Saying
that Perl remediates C syntax because it uses curly braces and
semi-colons under-emphasizes Perl's own syntax. Rather it seems
more evocative to describe ways in which Perl hybridizes elements
of C's grammar while augmenting them with grammar of its own.

In other words, to say that

\starttyping
my $variable = "value";    # defining a variable in Perl
\stoptyping

is a remediation of

\starttyping
char[5] variable = "value"; /* defining a variable in C */
\stoptyping

is an over-simplification. It obfuscates significant algorithmic
differences in the two approaches by focusing on the surface level
syntax (which is relatively similar) over the significant internal
differences in the way the two languages deal with variables (such
as static versus dynamic typing). The grammar of C is hybridized by
Perl---re-implemented rather than remediated, related yet
irreconcilable. Implementation differences have huge implications
on the utility and functionality of the languages, a theoretical
framework that focuses on surface-level similarities is incapable
of expressing the variation that occurs beneath those similarities.

Rather than a remediation of regular expressions, then, we see a
hybridization of specific grammars of regular expressions, with the
most popularly hybridized grammar deriving from the version found
in Perl. However, many of the languages that hybridize the Perl
version of regular expressions only implement a particular subset
of that version. Additionally, extensions may be added that are not
included in Perl. The result is a proliferation of regular
expression grammars as they are integrated into various process
hybridities such as programming languages, command line utilities,
and text editors.

The website {\em Rubular} stands as an example of how far-reaching
the hybridization of regular expressions has come in terms of
process assemblage complexity (Lovitt 2010). The website utilizes
not only the HTTP protocol that drives the World Wide Web, it uses
AJAX in order to provide real-time representations of pattern
matching within a Ruby interpreter (of which there are many). The
GUI browser is involved in this assemblage by
design[\letterhat{}zen]. So is a web framework of some kind,
running in the Ruby interpreter. There could be an argument made
against such far-reaching hybridity: Ruby can be programmed
interactively, line by line, in it's interpreter. The layers of
code wrapped around the processing of Ruby regexes could be seen as
superfluous---in fact, this is a common attitude of certain hacker
types who look with disdain upon any non-essential fuctionality.
Questions of {\em essentiality} in software remain an
under-discussed topic in new media studies, despite the
ever-present debates among developers on the issue. In this case,
remediation seems to be at play. By hybridizing regular expressions
with a Web 2.0 interface, the example of {\em Rubular} demonstrates
perfectly the urge for immediacy described by Bolter and Grusin,
even to the extent of real-time \quotation{Match/No Match}
evaluation of the regular expression that one is testing.

However, there is a conflict between the apparent remediation of
regular expressions from the CLI to the Web and the fact that both
the web site and the regular expressions are running in a Ruby
interpreter. From a process-oriented perspective, this is simply a
hybridization of regular expressions a new organizing logic that
presents a Web 2.0 interface.

Regular expressions are a specific grammar of process that provide
the functionality that enables the generative typesetting workflow
of this thesis. They are imperative in the specific, as they allow
me to reconfigure the original Markdown source into two
intermediate Markdown files each tailored for conversion to a
respective output format. They allow me to integrate my own markup
specificities, such as \type{(%grrrquote)}, which translates to
\type{<blockquote>} and \type{\startlongquote} in the intermediate
files. This is ugly markup, with a silly name, yet nevertheless it
allowed me to work around a serious issue I had with putting longer
quotes into the thesis.

Yet the organizational logic of regular expressions result in much
deeper implications for the metastability.

\startlongquote
Like calculus (which McLuhan considered a conquest of the tactile area of numbers) regular expressions anticipate the unpredictable and bring repeatability to the immeasurable. A simple * (which means "zero or more of the preceding item") compresses everything from zero to infinity into a calculable scheme... while text parses and subdivides thought, * dissolves and absorbs all text. Gutenberg separated oral speech into figure and ground, but * combines them again. Like the electron in its post-Newtonian atom shell, * ranges freely and resides nowhere. (Oram 2002)
\stoplongquote

This calculability enables not only a crucial substrate of this
thesis (Markdown): my effort to generate both HTML and PDF (through
ConTeXT) from a single source relies entirely on the hybridization
of regular expressions. Knowledge of Haskell would have allowed a
deeper integration with Pandoc that would have reduced or perhaps
even eliminated this top-level reliance on regular expressions.
Regardless, the organizational logic of Markdown arose as a result
of the transduction of regular expressions and their hybridization
into the Perl programming language in which the first Markdown
translation tool was written.

\section{The Materiality of this Thesis is a Hybridization of Process}

The problem domain from which this thesis emerged was a practical
consideration of cross-media publishing that came into play as soon
as I began to contemplate the various output options for arranging
text on the screen. The semantic qualities of HTML have clear
benefits from the standpoint of machine readability and material
malleability. Yet not once during the writing of this thesis have I
been contradicted when broaching the observation that the best
option we currently have for reading text on the screen is PDF, a
format that was designed to contain documents intended for
printing. The organizing logic of this practice-based approach is
to have the best of both worlds. That such a project spans the
materiality of this thesis across not only PDF and HTML but also
Markdown, {\CONTEXT}, templates for use by Pandoc, and Ruby code
provides a distinct means of questioning existing conceptions of
media.

\chapter{Conclusion}

Through an investigation into the conditions that define and enable
generative design, this thesis has interrogated traditional
definitions of media based on specificity and effect. A significant
problem with the theories of McLuhan, Bolter and Grusin, and
Manovich is their discursive presupposition that a \quote{medium}
is an easily-distinguished phenomenon. The ramifications of this
presupposition is a murky understanding of what constitutes a
medium. Rather than attempt to codify and enforce a set of criteria
for media, I have instead opted to describe the dynamics underlying
and enfolded by the on-going {\em becoming} of the computer
metamedium.

These dynamics were articulated in terms of
{\em processes hybridized behind the organizational logic of processual grammars}
and the unfolding of these hybridizations were shown to fit the
framework of Gilbert Simondon's ontogenesis. The concepts
themselves map onto the framework as well. {\em Process hybridity}
reflects that the solution provided by a transduction
\quotation{extracts the resolving structure from the tensions of the domain,}
while {\em grammars of process} expresses the reorganization of
those terms into a \quotation{concrete network} that excludes
nothing of its origins while at the same time restructuring and
modifying the domain of possibility within a system (Simondon 2009:
12).

The Markdown format exists because the processual grammar of
regular expressions could be organized in order to process
plain-text files for simple markup rules that map onto the
semantics of HTML. After its development it became increasingly
hybridized into other processual grammars, indicating an on-going
individuation. This type of continuous transduction implies that
the metastability surrounding the interaction of human and digital
process exists in a deeply energetic state from which new
transductions occur at a high rate. Simondon says that the source
within an individual for
\quotation{future metastable states from which new individuations can emerge}
is the result of a link to its preindividual nature (12). This
raises the question of whether the high \quote{transductivity} of
Markdown is linked causally to its emergence from within the highly
flexible domains of regular expressions and the computer
metamedium.

Gilbert Simondon's theory of ontogenesis also offers a
{\em non-deterministic} means for describing change that
incorporates both the human and the digital in its consideration.
This capacity for non-deterministic discussion is obviously useful
in media theory, where the debate between technological and
cultural determinism has raged for decades. Yet it also
{\em simply fits} where other theoretical frameworks fail to
encompass the dynamics involved in the evolution of the metamedium
and the multitude of processual grammars that form the conditions
of generative design.

The shift in 70 years from the materially and conceptually
cumbersome programming of computers with wires to the types of
hybridization enabled and exemplified by regular expressions
demonstrates an on-going and extremely productive individuation of
the computer. The degree of change is almost staggering. A key
aspect of this thesis has been to firmly situate humanity as deeply
{\em in} the metastability that encompasses both human and digital
processes. As I stated in the beginning, there is no digital
process that does not have, at its origin, a human process. For a
machine to catch a glitch, it must first be developed and built.

This thesis suggests that considering the metamedium of the
computer through process-based ontologies offers a different
perspective on how software is, and can be, organized. This
perspective has implications that are economic, social, cultural,
and political given the processes of collective becoming set in
motion through software-based engagements. These engagements are
expressed through the organizational logics of re-configurable
grammars of process capable of orienting metapotentials into unique
and radical individuations, if only we direct ourselves to
participating in organizing such a grand becoming.

\startworkscited

Priest, Dana and William M. Arkin. \quotation{Top Secret America}.
{\em Washington Post}. Web.
\letterless{}\useURL[1][http://projects.washingtonpost.com/top-secret-america/][][http://projects.washingtonpost.com/top-secret-america/]\from[1]\lettermore{}
(last accessed 8 August 2010).

PBS. (1996). \quotation{Triumph of the Nerds, Part Three}.
Transcript of video documentary.
\letterless{}\useURL[2][http://www.pbs.org/nerds/part3.html][][http://www.pbs.org/nerds/part3.html]\from[2]\lettermore{}
(last accessed 5 August 2010).

{\em Regular expression}. (2010). \quotation{Regular expression}.
{\em Wikipedia}. Web.
\letterless{}\useURL[3][http://en.wikipedia.org/wiki/Regular_expression][][http://en.wikipedia.org/wiki/Regular\letterunderscore{}expression]\from[3]\lettermore{}
(last accessed 28 July 2010).

Rosenberg, Scott. (2008). {\em Dreaming in Code}. New York: Three
Rivers Press. Print.

Simondon, Gilbert. (2009).
\quotation{The Position of the Problem of Ontogenesis}. Gregor
Flanders, trans. {\em Parrhesia} 7. PDF.
\letterless{}\useURL[4][http://www.parrhesiajournal.org/parrhesia07/parrhesia07_simondon1.pdf][][http://www.parrhesiajournal.org/parrhesia07/parrhesia07\letterunderscore{}simondon1.pdf]\from[4]\lettermore{}
(last accessed 5 June 2010).

Slashdot. (2007). \quotation{Are 80 Columns Enough?}.
{\em Slashdot}. 7 July 2007.
\letterless{}\useURL[5][http://ask.slashdot.org/article.pl?sid=07/07/07/1931246][][http://ask.slashdot.org/article.pl?sid=07/07/07/1931246]\from[5]\lettermore{}
(last accessed 14 August 2010).

% we should have an open works-cited going
\stopworkscited

\stoptext

