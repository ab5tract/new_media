<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <title>Grammars of Process</title>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="author" content="John C. Haltiwanger" />
  <meta name="date" content="" />

<!-- personal style options -->

	<style type="text/css">
		div[id="body_text"] {
			width: 61.8%;
			float: left;
			left: 15%;
			position: relative;
		}

		#footer {
		
		}

		a {
			text-decoration: none;
			color: black;
		}

		a:hover {
			text-decoration: underline;
		}
		/* we will need to add abbr's automatically */
		abbr { letter-spacing: 0.1em }

		body {
			font-family : "Lib-Sans-R", serif;
		}
		
		
		
		p {
			text-align: left;
			size: 12pt;
			line-height: 1.5;
			margin-bottom: 0;
		}

		em {
			font-family : "Lib-Serif-I";
		}

		h1[class="title"] {
			font-size : 2.8em;
			color : #910A00;
			right: 85%;
		}

		h1[class="author"] {
			font-size : 1.8em;
		}

		h1[class="date"] {
			font-size : 1.3em;
		}

		h1, h2, h3, h4, div#TOC {
			font-family : "Lib-SC";
			font-size : 12pt;
		}

		div#body_text > h1, h2, h3, h4 {
			position: relative;
			left: -2.5%;
		}

	/*	div[id="TOC"] > */ ul {
			list-style-type: none;	
		}

		div[id="TOC"] {
			line-height: 1.75;
		}

		div[class="title_section"] {
			padding-bottom : 4em; 
		}

		@font-face {
			font-family : "Lib-Serif-R";
			src : url(ttf/LinLibertine_Re-4.7.5.tt) format("truetype");
		}
/*
		@font-face {
			font-family : "Lib-Serif-I";
			src : url(ttf/) format("truetype");
		} */

		@font-face {
			font-family : "Lib-Serif-I";
			src : url(ttf/LinLibertine_It-4.2.6.ttf) format("truetype");
		}

		@font-face {
			font-family : "Lib-SC";
			src : url(ttf/LinLibertine_C-4.0.4.ttf ) format("truetype")
		}
	</style>

<!-- end personal style segment -->

</head>
<body>
<div class="title_page">
	<h1 class="title">Grammars of Process</h1>
	<h1 class="subtitle">Mapping Individuation in Human and Digital Processes Through Generative Design</h1>
	<h1 class="author">John C. Haltiwanger</h1>
	<h1 class="date"></h1>
	<p class="abstract">
		<h3 class="abstract_title">Abstract</h3>
		Despite years of theorization, a concise definition of what constitutes a medium remains elusive. Theorists have variously described media as extensions of human senses (Marshall McLuhan), as agents of reform ruled by a double-logic of remediation (Jay David Bolter and Robert Grusin), as aggregates of material specificity (N. Katherine Hayles), and as evolutionarily selected forms defined by their effects (Lev Manovich). While all of these theorists use examples of and from specific media, none of them explicitly address the specificities of media themselves. This thesis proposes a {\em process-oriented perspective} that seeks to address this slipperiness that has resulted from unclear definitions of the concept---a slipperiness that only intensifies within the context of the computer metamedium. Media are seen as reflexive sites in which humans create grammars that organize and distribute processes. As reflexive sites, they both change and are changed by human beings. The reflexiveness of the computer metamedium, defined as it is by its programmability, inspires an investigation into the dynamic and co-evolving relationship of human and digital process. The practice of {\em generative design} is selected as an evocative instance of this relationship, and a reflexive engagement with the practice is undertaken as the thesis becomes a site of generative typesetting. The concept of process utilized here is organized according to Gilbert Simondon's theory of {\em ontogenesis}, a framework that questions 'becoming' rather than 'being' and in so doing provides a mechanism for explaining collective change.
	</p>
</div>

<div id="TOC"
><ul
  ><li
    ><a href="#acknowledgments"
      ><span class="toc-section-number"
	>1</span
	> Acknowledgments</a
      ></li
    ><li
    ><a href="#introduction"
      ><span class="toc-section-number"
	>2</span
	> Introduction</a
      ><ul
      ><li
	><a href="#marshall-mcluhan-the-tetrad-and-the-temperature-of-media"
	  ><span class="toc-section-number"
	    >2.0.1</span
	    > Marshall McLuhan, the Tetrad, and the Temperature of Media</a
	  ></li
	><li
	><a href="#bolter-and-grusins-double-logic-of-remediation"
	  ><span class="toc-section-number"
	    >2.0.2</span
	    > Bolter and Grusin's Double-Logic of Remediation</a
	  ></li
	><li
	><a href="#n.-katherine-hayles-and-medium-specificity"
	  ><span class="toc-section-number"
	    >2.0.3</span
	    > N. Katherine Hayles and Medium Specificity</a
	  ></li
	><li
	><a href="#lev-manovich-and-medium-hybridity"
	  ><span class="toc-section-number"
	    >2.0.4</span
	    > Lev Manovich and Medium Hybridity</a
	  ></li
	><li
	><a href="#process-oriented-perspective"
	  ><span class="toc-section-number"
	    >2.0.5</span
	    > Process-Oriented Perspective</a
	  ></li
	><li
	><a href="#generative-design-begins-with-words"
	  ><span class="toc-section-number"
	    >2.0.6</span
	    > Generative Design Begins With Words</a
	  ></li
	><li
	><a href="#processes-within-the-borderland"
	  ><span class="toc-section-number"
	    >2.0.7</span
	    > Processes Within the Borderland</a
	  ></li
	><li
	><a href="#reflexive-methodology"
	  ><span class="toc-section-number"
	    >2.0.8</span
	    > Reflexive Methodology</a
	  ></li
	><li
	><a href="#origins-of-this-thesis"
	  ><span class="toc-section-number"
	    >2.0.9</span
	    > Origins of This Thesis</a
	  ></li
	></ul
      ></li
    ><li
    ><a href="#screens-and-controls"
      ><span class="toc-section-number"
	>3</span
	> Screens and Controls</a
      ></li
    ><li
    ><a href="#transduction"
      ><span class="toc-section-number"
	>4</span
	> Transduction</a
      ></li
    ><li
    ><a href="#operating-systems"
      ><span class="toc-section-number"
	>5</span
	> Operating Systems</a
      ></li
    ><li
    ><a href="#crystalized-process:-text-that-typesets-itself"
      ><span class="toc-section-number"
	>6</span
	> Crystalized Process: Text That Typesets Itself</a
      ><ul
      ><li
	><a href="#environment-of-operation"
	  ><span class="toc-section-number"
	    >6.1</span
	    > Environment of Operation</a
	  ><ul
	  ><li
	    ><a href="#constraints"
	      ><span class="toc-section-number"
		>6.1.1</span
		> Constraints</a
	      ></li
	    ><li
	    ><a href="#a-pre-format-necessarily-complicates-while-it-simplifies"
	      ><span class="toc-section-number"
		>6.1.2</span
		> A pre-format necessarily complicates while it simplifies</a
	      ><ul
	      ><li
		><a href="#regular-expressions-and-process-hybridity"
		  ><span class="toc-section-number"
		    >6.1.2.1</span
		    > Regular Expressions and Process Hybridity</a
		  ></li
		></ul
	      ></li
	    ></ul
	  ></li
	></ul
      ></li
    ><li
    ><a href="#conclusion"
      ><span class="toc-section-number"
	>7</span
	> Conclusion</a
      ></li
    ><li
    ><a href="#bibliography"
      ><span class="toc-section-number"
	>8</span
	> Bibliography</a
      ></li
    ></ul
  ></div
>



<div id="body_text">
<div id="acknowledgments"
><h1
  ><a href="#TOC"
    ><span class="header-section-number"
      >1</span
      > Acknowledgments</a
    ></h1
  ><p
  >The final shape of this thesis is deeply indebted to those who have helped me along the way. First I would like to thank Richard Rogers. Without his gentle-yet-forceful pressure to elevate the discussion contained within my thesis, I fear that the project would ultimately demonstrate little of whatever theoretical power it currently enjoys. Likewise, without Geert Lovink the project would not exist in the first place, as it was his request for a post-journal publishing platform that started me on this quest of generative typesetting for multiple output formats. Thanks also to Florian Cramer, not least for explaining that simply typesetting the thesis in two formats was enough technical work for a single-year masters thesis but also for providing a strong perspective from which to begin. I am deeply indebted to Michael Jason Dieter for introducting me to the work of Gilbert Simondon, without whose theory this thesis would not be feasible. Huge thanks go to Femke Snelting and Pierre Huyghebaert for their indispensible interview and for welcoming me into their extremely creative network. For issues with JavaScript in the HTML version of this thesis, Michael Murtaugh once again proves his potency as a programmer. Special thanks to my colleagues at the UvA, for helping me to stay sane in the midst of an insane project: Hania Pietrowska, Natalia Sanchez, Rakesh Kanhai, Sarah Moore, Morgan Curry, Marc Stumpel, Allison Guy, and Ramses Petronia. Every day I grow to further realize that without my friends, I am nothing. Finally, a sincere thanks to all those who have shared insights and conversations with me as I pushed to develop my ideas further.</p
  ><p
  >And last, but never least, my family. Thank you mom, dad, Brett, Julia, and Ann. Thank you for your unwavering support!</p
  ></div
><div id="introduction"
><h1
  ><a href="#TOC"
    ><span class="header-section-number"
      >2</span
      > Introduction</a
    ></h1
  ><p
  >Today's new media theory increasingly invokes <em
    >materiality</em
    > as a significant, perhaps even <em
    >the</em
    > significant, mode of investigating digital objects and the media through which they are delivered. This thesis questions such a centrality of materiality through a practice-based, process-oriented approach. <em
    >Process</em
    > is proposed as the atomic unit of that which new media theory investigates. This is true on a formal material level: applications run as either as individual process or as assemblages of process which are managed by an operating system and through which the application's code is accomplishes all of its tasks, from memory and access to algorithmic execution on the central processing unit. A process-oriented approach will be shown to provide an alternative methodology for engaging with and understanding software that may be more productive than what a material analysis alone provides. For instance, certain problematics within Lev Manovich's concept of 'media hybridity' will be resolved by a re-orientation towards process (Manovich 2008). Process also allows a fresh perspective for examining human-digital relations. Human processes and digital processes are seen as inextricably intertwined, leaving any discussion of digital process that excludes relevant dimensions of human process necessarily unfinished.</p
  ><p
  >Process has recently elevated as a focal point within the design world as more and more designers switch to, or otherwise integrate, generative workflows. <em
    >Generative design</em
    > is a form of design in which software algorithms are used from the bottom up, through source code that directs all drawing and manipulation of the objects the source describes. This is opposed to the top-down, What You See Is What You Get (WYSIWYG) style of design embodied in the industry-standard applications from Adobe. New media theorist Florian Cramer has identified generative design as the cutting edge of design in the Netherlands (Cramer, Monsoux, and Murtaugh 2010). The emergence of generative design as a widespread practice is reflected by the Breda-based Graphic Design Museum's decision to host an installation of generative works called <em
    >InfoDecoData</em
    >, as well as a symposium of the same name.<sup
    ><a href="#fn1" class="footnoteRef" id="fnref1"
      >1</a
      ></sup
    ></p
  ><p
  >This space of generative design provides an ideal site for investigating questions of materiality and medium-specificity within the computer <em
    >metamedium</em
    > for a number of reasons. The importance of source code to generative design intersects with an on-going dialog within software studies concerning where to position code in a study. Its position close to the cutting edge of what is being done with computers invites inquiry into the processes that are assembled to compose today's computers--<em
    >screens, controls, and operating systems</em
    >. Finally, the tendency of generative design to involve the command-line interface compels an investigation into this under-theorized medium, and with it the sustained primacy of text within computer interfaces. This primacy of text is reflected all the way from source code to the labels on buttons and menus in a GUI interface.</p
  ><p
  >Before explaining in more detail the process-oriented perspective utilized by this thesis, however, it first seems prudent to visit some of the dominant strains of medium theory found in the history of media studies which will be interrogated in this thesis.</p
  ><div id="marshall-mcluhan-the-tetrad-and-the-temperature-of-media"
  ><h3
    ><a href="#TOC"
      ><span class="header-section-number"
	>2.0.1</span
	> Marshall McLuhan, the Tetrad, and the Temperature of Media</a
      ></h3
    ><p
    >Marshall McLuhan's prominence in media studies is in no small part related to the pionering role he had in shaping the field. Beginning with his work in analyzing advertising, published in 1951 as <em
      >The Mechanical Bride</em
      >, McLuhan critically integrated the language of an advertising industry that had developed a vocabulary for considering their new role in targetting not just print but also television and radio. This critical integration allowed for a consideration of media on a level that had not, up to that point, emerged. That is to say, it was his seminal work <em
      >Understanding Media</em
      > that proposed that it is the study of <em
      >media themselves</em
      >, and not simply their content, that is necessary for critical engagement.</p
    ><p
    >One of the means he proposed as a distinguishing characteristic between media was the &quot;temperature&quot; of a given medium. Though the wording is perhaps somewhat un-intuitive, McLuhan defines &quot;hot&quot; media as those media forms which extend &quot;one single sense in 'high definition'&quot; (McLuhan 1964: 24). The effect of such extension is a reduction in the involvement of the audience to the medium. A photograph, for instance is &quot;hot,&quot; while a cartoon is &quot;cool&quot; because it contains relatively sparse amounts of visual information. The phonetic alphabet, McLuhan argues, is a &quot;hot and explosive&quot; medium, with vastly different effects than the cool medium of ideogrammic writing (25). In his view, he transformation of this alphabet by the printing press, this &quot;hotting-up&quot; of the writing, led to &quot;nationalism and the religious wars of the sixteenth century&quot; (25). The temperature of a medium interacts with the temperature of a culture in a way that necessarily redefines the culture (and, it could be argued, the medium as well, which may decrease in effective temperature as the temperature of a culture rises in relation). For instance, the &quot;hot radio medium used in cool or nonliterate cultures has a violent effect, quite unlike its effect, say in England or America, where radio is felt as entertainment&quot; (33). For this and similar views, Mcluhan received condemnations as a 'technological determinist,' someone who argues that technology defines culture.</p
    ><p
    >His concept of the Tetrad displays a bit more subtlety of thinking. This concept is summarized and, it is claimed, validated in light of contemporary times by Arthur and Marilouise Kroker in their text &quot;Code Drift&quot;:</p
    ><blockquote>[F]or McLuhan all new media simultaneously render an older form obsolescent, represent something fundamentally new, retrieve the superceded form of an older media as a cultural masquerade to make what's really new more acceptable, and when put under extreme pressure reverse to their opposite. (Kroker and Kroker 2010).</blockquote>
<p
    >McLuhan's Tetrad is a sort of physics for media, a description of a dynamic through which all media undergo upon their development and introduction. This is a departure from pure techno-determinism because in the end it is human processes which develop new media forms and embed older forms within them---it is human processes that expect or demand that new media &quot;masquerade&quot; as older media.</p
    ></div
  ><div id="bolter-and-grusins-double-logic-of-remediation"
  ><h3
    ><a href="#TOC"
      ><span class="header-section-number"
	>2.0.2</span
	> Bolter and Grusin's Double-Logic of Remediation</a
      ></h3
    ><p
    >Jay David Bolter and Richard Grusin take this question of what actually drives the physics described by the Tetrad. Their conclusion is a form of double-logic that envelopes one of the key elements of the Tetrad--that new media forms embed the older ones they replace, a process which they termed <em
      >remediation</em
      >--and explains it as the result of two interacting forces, <em
      >immediacy</em
      > and <em
      >hypermediacy</em
      >. The logic of immediacy seeks to erase the &quot;medium-ness&quot; of a medium. This can be seen in the drive for &quot;transparent&quot; interfaces: &quot;a transparent interface is one that erases itself, so that the user would no longer be aware of confronting a medium, but instead would stand in an immediate relationship to the contents of a medium&quot; (Bolter and Grusin 1996: 318). Hypermediacy, on the other hand, &quot;acknowledges multiple acts of representation and makes them visible&quot; (328). Hypermediacy is, then, an opposing vector, one that delights in highlighting the presence of the medium rather than attempting to obfuscate or &quot;disappear&quot; it. The root cause of this process of double-logic, in the end, is the desire to &quot;achieve the real&quot;:</p
    ><blockquote>Hypermedia and transparent media are opposite manifestations of the same desire: the desire to get past the limits of representation and to achieve the real. They are not striving for the real in a metaphysical sense. Instead, the real is defined in terms of the viewers experience: it is that which evokes an immediate (and therefore authentic) emotional response. Transparent digital applications seek to get to the real by bravely denying the fact of mediation. Digital hypermedia seek the real by multiplying mediation so as to create a sense of fullness, a satiety of experience, which can be taken as reality. (Bolter and Grusin 1996: 343)</blockquote></div
  ><div id="n.-katherine-hayles-and-medium-specificity"
  ><h3
    ><a href="#TOC"
      ><span class="header-section-number"
	>2.0.3</span
	> N. Katherine Hayles and Medium Specificity</a
      ></h3
    ></div
  ><div id="lev-manovich-and-medium-hybridity"
  ><h3
    ><a href="#TOC"
      ><span class="header-section-number"
	>2.0.4</span
	> Lev Manovich and Medium Hybridity</a
      ></h3
    ></div
  ><div id="process-oriented-perspective"
  ><h3
    ><a href="#TOC"
      ><span class="header-section-number"
	>2.0.5</span
	> Process-Oriented Perspective</a
      ></h3
    ><p
    >In his outlining of the nature of a 'processual media theory,' Ned Rossiter asserts that &quot;a processual media theory examines the tensions and torques between that which has emerged and conditions of possibility; it is an approach that inquires into the potentiality of motion that underpins the existence and formation of a system&quot; (2007: 178). While the argument that Rossiter builds in his chapter on the subject revolves mainly around interrogating and the processes that drive new media within its institutional settings, the thrust of his argument--that new media empirics must &quot;reflexively engage with the field of forces that condition its methodology&quot;--maps easily to a more general line of inquiry (171). This is perhaps best embodied in his explanation of a 'processual aesthetics of new media'<sup
      ><a href="#fn2" class="footnoteRef" id="fnref2"
	>2</a
	></sup
      >:</p
    ><blockquote>A processual aesthetics of new media goes beyond what is simply seen or represented on the screen. It seeks to identify how online practices are always conditioned by and articulated with seemingly invisible forces, institutional desires and regimes of practice. (Rossiter 2007: 174)</blockquote>


<p
    >While a truly exhaustive investigation of the multitude of relations--social, economic, technological, ideological--involved in generative design--is simply not within the scope of this thesis, I believe that it is important to begin filling in the &quot;gap&quot; between source code and execution. A process-oriented perspective encourages this by first defining a given materiality as an <em
      >assemblage of process</em
      >. This is as true in the physical world as it is in the digital, as all that exists has taken its shape as a result of <em
      >becoming</em
      >. From this recognition, this process-oriented perspective integrates the theoritization of ontogenesis proposed by Gilbert Simondon. This theory will be described in further detail in a later section (!WHICH!). For now, however, it is best to explain that in Simondon's ontogenesis<sup
      ><a href="#fn3" class="footnoteRef" id="fnref3"
	>3</a
	></sup
      >, change in a system is the result of incongruency within that system. In other words, difference causes change.</p
    ><p
    >To test this assertion, I survey historical developments within computing as well as practices of contemporary generative design and typesetting with open source software. Where do changes--or, in Simondon's language, <em
      >transductions</em
      >--occur within the domain of computation? How did the assemblages of process upon which generative design rely <em
      >become</em
      > what they are today? The historical survey demonstrates that changes in computers do arise in response to &quot;differences&quot;--that is to say, problems. These problems can be 'real', 'virtual', 'imaginary', or otherwise. The solutions to these problems are influenced by the structure of relations within which the problems themselves arise. This observation demonstrates that Simondon's description of ontogenesis is sound, as well as applicable to describing the dynamics of the metamedium.</p
    ><p
    >The second stage of the process-oriented perspective seeks to interrogate and critique the mode of existence of <em
      >mediums</em
      > within the computer metamedium. When materiality within the computer is defined as 'assembled process,' mediums lose their seeming rootedness in materiality---that is, unless everything assembled within the computer is deemed a medium. To make this point clear: if a medium is defined by its material specificities, and within a computer every non-identical digital object contains--or is embedded within--particular specificities, then every unique digital object (or any unique application used to handle it) has the potential to be labelled a 'medium'. Variations in interface, say the difference between Windows Explorer and Mac OS X's Finder can be said to hold real, material specificities. Labelling them 'mediums', however, opens an un-closeable box through which every digital process that maintains its own interface becomes a medium. In my opinion, this is unacceptable.</p
    ><p
    >A separate element, outside of material specificities, must be invoked in order to explain and discuss mediums based within the computer metamedium. In response to this question I identify the existence of <em
      >grammars of process</em
      > which enable, require, and inspire the assembling of process. Combined with a sufficiently conservative definition of 'medium,' these processual grammars provide space for discussing the vast variation in material specificities without diluting the term 'medium' into a troubling meaninglessness. While not entirely sufficient to resolve the boundaries of medium and not-medium, this concept of processual grammar nevertheless provides at least a tool for articulating and guiding discussions along those lines.</p
    ></div
  ><div id="generative-design-begins-with-words"
  ><h3
    ><a href="#TOC"
      ><span class="header-section-number"
	>2.0.6</span
	> Generative Design Begins With Words</a
      ></h3
    ><p
    >The first is the 'workspace' of much of generative design: plain-text source files. These source files often, though not exclusively, undergo their generation within a command-line interface (CLI). The command-line interface is defined by the primacy of text in its workings. The centrality of text to generative design invites a corrective movement against the general lack of focus on the processes behind typesetting among new media theory. While the surfaces of text and textual interfaces have been investigated in numerous ways (Bolter 2001; Fuller 2000), there has been a general lack of theoretical concern regarding the underlying processes of text in the metamedium. As opposed to the overflowing amount of literature relating to visually-rich computer interfaces, very little theory has been written regarding the command-line---despite its place as the historical interface (once contemporaneous with punch cards) by which digital processes were initiated. Far from being obsolete, both Microsoft and Apple ship command line interfaces within their operating systems. In Microsoft's case, significant money has been spent developing a new grammar and implementing new functionalities into their modern command line implementation Powershell. Likewise, Google found the command-line relevant enough to release a tool for interacting with its online services from that interface (Holt and Miller 2010).</p
    ><p
    >This centralization of text and the command-line raises pertinent questions that may help to clear up the almost-hopelessly fuzzy nature of materiality in the computer metamedium. What aspects define the materiality of the command-line? What are its medium-specificities? Do existing theories such as <em
      >remediation</em
      > apply to the workings of the command-line? What differences exist between various command-lines?</p
    ><p
    >Furthermore, what processes have assembled in order to form the context of modern-day command-lines? Such a question delves into the origins of the personal computer and its perceived significance both before and after its introduction. In Alan Kay's vision of the <em
      >computer as metamedium</em
      >, the system supports and encourages the instantiation of new media forms by individuals who have no formal background in programming (Kay 1977). Apple's Macintosh famously delivered the vast majority of the human-computer interface innovations developed by Kay's team at Xerox PARC labs---without this key feature of easy programmability.<sup
      ><a href="#fn4" class="footnoteRef" id="fnref4"
	>4</a
	></sup
      > The orginal Macintosh operating system also presented its total lack of a command-line as an ideal formulation.</p
    ><p
    >When Apple introduced Mac OS X a decade and a half later, its Unix underpinnings--complete with command-line--appeared in marketing as a selling point. For almost a decade the adoption of Apple computers has risen continuously. In chapter two, Mac OS X is described as a particular hybridization of process that both intersect with and reflect &quot;external&quot; processes such as economics, inertia, and ideology.</p
    ></div
  ><div id="processes-within-the-borderland"
  ><h3
    ><a href="#TOC"
      ><span class="header-section-number"
	>2.0.7</span
	> Processes Within the Borderland</a
      ></h3
    ><p
    >The centrality of source code itself--rather than the centrality of the source code as text--to generative design provides a second compelling reason for investigating materiality and medium-specificity within the metamedium. Currently at issue within the emerging field of software studies is where to responsibly place source code in its investigations. Some propose, as Lev Manovich does in his <em
      >The Language of New Media</em
      >, that understanding the logic of new media requires investigating the field of computer science for the &quot;new terms, categories, and operations that characterize media that become programmable&quot; (Manovich 2001: 48). This is a call to software as <em
      >logos</em
      >. It attempts to solidify theory by giving it a specific direction--the logic and objects of software.</p
    ><p
    >Wendy Hui Kyong Chun, however, questions this direction, criticizing the erasure of &quot;the vicissitudes of execution and the structures that ensure the coincidence of code and its execution&quot; that results when one elevates source code--and by extension software--as a totalizing logic (Chun 2008: 303). When theorists such as Alexander Galloway argue that source code is &quot;automatically executable,&quot; they fetishize source code by collapsing source code with the effects of that code's execution. In other words, the execution itself is erased, along with the conditions buttressing that execution. Rather than approach a project of solidifying theory (that is, ending &quot;vapor theory&quot; as advocated by Geert Lovink and Galloway, among others) through reducing the computer metamedium to the code that it runs, Chun advocates an approach of code as a <em
      >re-source</em
      >, a perspective which &quot;enables us to think in terms of the gap between source and execution&quot; (321). This gap seemingly includes the &quot;borderland&quot; in which N. Katherine Hayles positions materiality, &quot;the connective tissue joining the physical and mental, the artifact and the user&quot; (Hayles 2004: 72). That Chun identifies the 'code as re-source' perspective as positioning an &quot;interface as a process rather than as a stable thing&quot; resonates with the process-oriented perspective proposed in this thesis (Chun 2006: 321).</p
    ><p
    >Similarly resonant is the recent series of lectures by David Crockford. While the series relates to the programming language JavaScript, Crockford utilizes his entire first lecture to describe the evolution of programming interfaces throughout the development of digital computers (Crockford 2010a). Starting from the &quot;spaghetti code&quot; of wires that provided the original means of programming, Crockford proceeds to explain in great detail the multitude of processes that defined programming prior to its current state. Integrating these observations with the memoir account of Severo Ornstein allows for an attempt to fill in historical elements that belong to this gap, or borderlands (Ornstein 2002). The historical evolution of these important sites of interaction between human and digital processes is seen as an overlooked aspect in both the field of computer science and in new media discourse. The relevance of <em
      >inertia</em
      > in the composition of human-digital processes is demonstrated by examining the surprisingly large number of elements that remain a part of computing that have no reason for present-day integration other than the weight of history. This seems an important aspect within the inter-relations of human and digital processes.</p
    ></div
  ><div id="reflexive-methodology"
  ><h3
    ><a href="#TOC"
      ><span class="header-section-number"
	>2.0.8</span
	> Reflexive Methodology</a
      ></h3
    ><p
    >In addition to the historical survey of digital processes and its attempt to map Gilbert Simondon's language onto that domain, this thesis has a separate case study: the typsetting of itself, in multiple output formats, through a generative workflow. This provides a secondary mechanism for interrogating Simondon's theory of ontogenesis. Central to ontogenesis is the question of <em
      >individuation</em
      >.</p
    ><p
    >By examining the underlying processes of presentation required to 'typeset' the text itself, this reflexive methodology further demonstrates the validity of classifying materiality as assembled process. Through the utilization of FLoSS software, multiple output formats will be not only be investigated but also materially instantiated through a designed mechanism of process--a <em
      >processual hybridity</em
      >. These output formats represent two of the top formats currently used to manage and display texts digitally: HTML and PDF. Questioning the materiality of the input format, Markdown, is a useful exercise: what is the materiality of a format whose use implies an intention to convert to--to exist as--a multitude of <em
      >other materialities</em
      >? This type of materiality (transitional, &quot;unfinished&quot;) runs throughout the field of generative design. By designing, documenting, and describing a genuine generative workflow I am able to integrate a significant degree of reflexivity into the thesis at a level of materiality as well as content.</p
    ><p
    >The environment employed in this project of generative typesetting is entirely composed of software for which I have access to the source. That is to say, it is composed entirely<sup
      ><a href="#fn5" class="footnoteRef" id="fnref5"
	>5</a
	></sup
      > &quot;Free/Libre/Open Source Software&quot;, here to be abbreviated as FLoSS.<sup
      ><a href="#fn6" class="footnoteRef" id="fnref6"
	>6</a
	></sup
      > Rather than delve deeply into a historical account of what I call here <em
      >ideological computing</em
      ><sup
      ><a href="#fn7" class="footnoteRef" id="fnref7"
	>7</a
	></sup
      >, I present an argument for the use of FLoSS based on the characterstics of represive and emancipatory media outlined by Hans Magnus Enzensberger in his essay &quot;Constituents of a Theory of the Media&quot; from 1970. Such a discussion seeks reflexivity into the ideology driving my own personal usage of FLoSS, rather than a reflexivity into the historical dynamics of ideological computing.</p
    ><p
    >Historical dynamics do emerge, however, as I discuss the specificities of the formats used in this project. As the &quot;content layer&quot; of the World Wide Web, HTML is the most pervasive, if not also the most well-known, markup language on the planet. It is defined in its specification as What processes does HTML hybridize in coming into existence? What specificities does it entail? What processes has it in turn inspired?</p
    ><p
    >The second output format, PDF, is actually achieved through the use of an intermediary format. This intermediary format is composed of constructions for a process hybridity called ConTeXt. In specific terms, ConTeXt is a macro package for the venerable typesetting program called TeX (this year celebrating its 32nd, or 2^8, birthday). ConTeXt is a fast-moving project that provides a unique site for assessing process hybridity.</p
    ></div
  ><div id="origins-of-this-thesis"
  ><h3
    ><a href="#TOC"
      ><span class="header-section-number"
	>2.0.9</span
	> Origins of This Thesis</a
      ></h3
    ><p
    >This thesis springs from a practical consideration, namely the issue of <em
      >cross-media publishing</em
      >, a consideration that came into play as soon as I began to contemplate the various options for arranging text on the screen. Not once during the writing of this thesis have I been contradicted when broaching the observation that the best option we currently have for reading text on the screen is PDF, a format that was designed to contain documents intended for printing. PDF, or Portable Document Format. However, disregarding issues of display and presentation, the HTML format underlying the World Wide Web provides significantly more malleability in relation to the text it contains. In light of calls within the field of new media for investigating <em
      >medium-specficities</em
      >, analyzing these formats seemed a prudent course. Likewise, the strident focus on <em
      >materiality</em
      > within the field becomes an open question as we involve a third format, Markdown, which was designed specifically to be translated into other formats.<sup
      ><a href="#fn8" class="footnoteRef" id="fnref8"
	>8</a
	></sup
      ></p
    ></div><div class='chapter'></div
  ></div
><div id="screens-and-controls"
><h1
  ><a href="#TOC"
    ><span class="header-section-number"
      >3</span
      > Screens and Controls</a
    ></h1
  ></div><div class='chapter'></div
><div id="transduction"
><h1
  ><a href="#TOC"
    ><span class="header-section-number"
      >4</span
      > Transduction</a
    ></h1
  ><p
  >In his text <em
    >The Position of the Problem of Ontogenesis</em
    >, Simondon writes,</p
  ><blockquote>By transduction we mean an operation--physical, biological, mental, social--by which an activity propagates itself from one element to the next, within a given domain, and founds this propagation on a structuration of the domain that is realized from place to place: each area of the constituted structure serves as the principle and the model for the next area, as a primer for its constitution, to the extent that the modification expands progresively at the same time as the structuring operation. (Simondon 2009: 11).</blockquote>
<p
  >Repurposed from the language of chemistry, Simondon's metaphorically images transduction with the example of a substrate--swelling with <em
    >metapotential</em
    >--that crystallizes. The final formation is the substrate fulfilling this metapotential, a fulfillment that arises only through an unpredictable unfolding involving emergent factors.<sup
    ><a href="#fn9" class="footnoteRef" id="fnref9"
      >9</a
      ></sup
    ></p
  ><p
  >&quot;Each new medium is justified because it fills a lack or repars a fault in the predecessor, because it fulfills the unkempt promise of an older medium&quot; (Bolter and Grusin 1996: 351).</p
  ></div><div class='chapter'></div
><div id="operating-systems"
><h1
  ><a href="#TOC"
    ><span class="header-section-number"
      >5</span
      > Operating Systems</a
    ></h1
  ></div><div class='chapter'></div
><div id="crystalized-process:-text-that-typesets-itself"
><h1
  ><a href="#TOC"
    ><span class="header-section-number"
      >6</span
      > Crystalized Process: Text That Typesets Itself</a
    ></h1
  ><p
  >The time has come to for the self-reflective approach of this thesis to come into play. For a book called <em
    >Writing Space</em
    >, this work by Jay David Bolter provides scant discussion of actual writing environments on the computer. Originally written before the expansion of the World Wide Web into the sphere of popular culture, <em
    >Writing Space</em
    > is concerns itself with &quot;the space of electronic writing&quot;.<sup
    ><a href="#fn10" class="footnoteRef" id="fnref10"
      >10</a
      ></sup
    > In defining this space as &quot;both the computer screen, where text is displayed, and the electronic memory, in which it is stored,&quot; Bolter belies the relative absence of process in materialist forms of media analysis (2001: 13).</p
  ><p
  >Bolter's over-simple definition of electronic writing space does not incorporate the act of writing, only the display of it. This surface-level analysis fits well the application of his remediation theory---the surface of a medium (it's &quot;screen&quot;) is the host site of remediation. Bolter delves below the surface in his explanation of a shift to topographical writing. In the electronic writing space, &quot;any relationships that can be defined as the interplay of pointers and elements&quot; are representable (32). The writing space &quot;itself has become a hierarchy of topical elements&quot; (32). This is the effect of the computer's affinity for symbol-processing: &quot;Any symbol in the space can refer to another symbol using its numerical address&quot; (30). To highlight the dimensional shift in the writing space, Bolter describes the operation of outline processors. These programs abstract a text to the level of sections. These sections can be moved around and manipulated. Writing becomes <em
    >topological</em
    > in the sense that they now have a spatial aspect. Sections of text have obtained a modularity and independence that allows visual arrangement.<sup
    ><a href="#fn11" class="footnoteRef" id="fnref11"
      >11</a
      ></sup
    > What he does not talk about is the formats generated by the Macintosh program that he uses in his figures. He mentions passing layouts around on floppies between friends, but that assumes a parity of operating systems and proprietary software (at least in his example) (!CITE!).</p
  ><p
  >(!!! WHERE WAS I GOING?? !!!)</p
  ><div id="environment-of-operation"
  ><h2
    ><a href="#TOC"
      ><span class="header-section-number"
	>6.1</span
	> Environment of Operation</a
      ></h2
    ><p
    >This text is not typed in the manner that you see it. The above header is instead written like this:</p
    ><pre
    ><code
      >  # Environment of Operation #
</code
      ></pre
    ><p
    >Through the wrapper program <code
      >pandoc</code
      >, this input (written in Markdown) is converted into HTML and ConTeXt outputs.</p
    ><dl
    ><dt
      >HTML</dt
      ><dd
      ><p
	><code
	  >&lt;h1&gt;Environment of Operation&lt;/h1&gt;</code
	  ></p
	></dd
      ><dt
      >ConTeXt</dt
      ><dd
      ><p
	><code
	  >\section{Environment of Operation}</code
	  ></p
	></dd
      ></dl
    ><p
    >The syntax of HTML represents a semantic operation: &quot;Dear Mr. Browser, treat this as a header of level 1.&quot; The syntax of ConTeXt, however, represents a macro command within a programming language. What it says is &quot;call the sections of code that translate the text within the brackets to the parameters specified for the <code
      >\section{}</code
      > command.&quot;</p
    ><p
    >The literal 'writing space' of this thesis is a program called Textroom. Textroom is a minimalist text editor in which there are no buttons, taskbars, or other clutter. Only you, your words, and (optionally) informational text reporting the time, word count, percentage to accomplishing your writing goal, etc. By writing in plain-text, I open myself to the opportunities afforded me by version control systems. Developed to enable collaboration of programmers on a code base, version control systems can track changes in text across time (useful for this project) and allow for massively distributed workflows involved tens of thousands of individuals (useful for the Linux kernel).</p
    ><div id="constraints"
    ><h3
      ><a href="#TOC"
	><span class="header-section-number"
	  >6.1.1</span
	  > Constraints</a
	></h3
      ><div class="figure"
      ><img src="images/two_editors.png" alt="Textroom and gvim. gvim is displayed on this screen but running on my netbook."
	 /><p class="caption"
	>Textroom and gvim. gvim is displayed on this screen but running on my netbook.</p
	></div
      ><p
      >Above you see a necessary adaptation within my workflow. My netbook took a fall and lost the ability to use its screen. Because of the nature of the command line, I was able to log in to computer and execute commands that allowed me to establish remote access using a piece of software called <code
	>ssh</code
	>. This remote connection can also support the transmission of GUI applications using the client-server model at the heart of the X Windows system (which drives the GNU/Linux GUI). This was important a) to get files off the netbook, and b) the version of <code
	>pandoc</code
	> on my desktop had stopped working after a modular dependency was upgraded and I was finding it impossible to upgrade <code
	>pandoc</code
	> itself. In the image above we see the minimal editor Textroom and the editor gvim. Because of the pandoc incompatibility on the desktop, I was forced to use the netbook as the site of typesetting. This shaped the output of the project most likely by time. However, the simple ability to log in and enable remote access shaped this project immensely by allowing me to extract important work that would otherwise have been lost.</p
      ><p
      >An unfortunate constraint is the inability to take advantage of elements of the TeX landscape that are reknowned for making life easier. The chief among these is BibTeX, which allows for a bibliography to be dynamically generated and citations to be inserted according to a variety of formats (that one can change with a single line of text, if desired). By abstracting myself from TeX by using Markdown as the &quot;pre-format,&quot; I've lost the opportunity to easily manage bibliographic data and instead must input it by hand. That said, the MLA format is not currently available in BibTeX meaning that--even if I could use this software--the output would be necessarily shaped by the constraints of the tools.</p
      ></div
    ><div id="a-pre-format-necessarily-complicates-while-it-simplifies"
    ><h3
      ><a href="#TOC"
	><span class="header-section-number"
	  >6.1.2</span
	  > A pre-format necessarily complicates while it simplifies</a
	></h3
      ><p
      >Using the 'markdown' pre-format complicates several issues with typesetting documents in both HTML and ConTeXt. For instance, ConTeXt includes a <code
	>\chapter{}</code
	> macro, which influences the numbering of sections so that sections are relative to the chapter number, rather than to the entire text. Thus the first section of chapter two will be rendered into text as '2.1'. The issue arises because HTML has no similar distinction: sections are related to the depth of the header, such that H1 is the highest level section (the equivalent of ConTeXt's <code
	>\chapter{}</code
	>). In markdown, the top-level section appears as in the example above, that is using a single '<code
	>#</code
	>'. My initial solution was to include two sections with only one '<code
	>#</code
	>' per chapter, the first of which I would manually change from <code
	>\section{}</code
	> to <code
	>\chapter{}</code
	>, with the result that the sectioning of the chapter fits what is expected---chapter 2, section 1 is numbered as '2.1'.</p
      ><p
      >This does not satisfy HTML, however, as there is nothing to change the first single '#' into: it is already inserting the highest level section, 'H1'. Thus, where ConTeXt begins numbering the sections within the chapter (2.1, 2.2, etc.), HTML increments the top-level section number, so that the second single '#' increments to '3', instead of to '2.1'. If a double '##' is used, the sectioning will appear as we desire in the HTML, but will appear as '2.0.1' in ConTeXt. This incompatibility requires either modifying the Pandoc source code directly or else the creation of a specific 'helper' script to create a sectioning parity between the two output formats. As I do not know Haskell, I've opted for the second solution.</p
      ><p
      >Since markdown allows passing TeX commands, I solved the problem by using <code
	>\chapter{}</code
	> to designate the chapter title. Then, using a command-line script written in the Ruby programming language, a copy of the markdown file is created in which <code
	>\chapter{}</code
	> is replaced by a single '#' and all subsequent '#'s are increased by one '#' until the next <code
	>\chapter{}</code
	> is reached. From this copy we generate the HTML version, while the original can be processed into TeX. Perhaps this technical description appears to be more of a computer science discussion than it is a media theory one. However it highlights the ways in which processes hybridize: the conflicting grammars of ConTeXt and HTML create a complication which the wrapper program Pandoc either does not or can not address. To work around this issue, a separate program (the Ruby interpeter) is used to integrate a script file which deals with the problem. This is a common feature of a command-line based workflow: &quot;glue&quot; scripts are written in order to fuse processes together. This glue can result from .</p
      ><div id="regular-expressions-and-process-hybridity"
      ><h4
	><a href="#TOC"
	  ><span class="header-section-number"
	    >6.1.2.1</span
	    > Regular Expressions and Process Hybridity</a
	  ></h4
	><p
	>Regular expressions represent another avenue for demonstrating process hybridity. Since their introduction into the early text editors <code
	  >QED</code
	  > and <code
	  >ed</code
	  > by Ken Thompson, regular expressions have since been incorporated into many Unix commands such as <code
	  >grep</code
	  > and <code
	  >awk</code
	  >, newer editors such as <code
	  >vi</code
	  > and <code
	  >emacs</code
	  >, and programming languages such as Perl, PHP, Python, Ruby, and many more (<em
	  >Regular expression</em
	  > 2010). Regular expressions are a means for describing parameters of text searches, whereby arranging esoteric control characters in and around the text one is looking to find allows for finely tuned pattern matching.</p
	><p
	>The hybridity of regular expressions lies in it's adoption by nearly every major programming language: from Wikipedia, the list includes &quot;Java, JavaScript, PCRE, Python, Ruby, Microsoft's .Net Framework, and the W3C's XML Schema&quot; (<em
	  >Regular expression</em
	  > 2010). This list of languages refers to those who have hybridized some form or derivative of Perl's implementation of regular expressions, which is considered more robust than Ken Thompson's original version.</p
	><p
	>Is it possible to say that these programming languages are 'remediating' the regular expressions from Perl? It is not beyond reason to assert that programming languages are 'mediums'---Ken Thompson has referred to Smalltalk as a new medium, for instance (!CITE!). However, mapping the term medium onto a programming language falls into the same trap of stretching the term medium until it becomes incomprehensible. Do different versions of the same language, representing different capabilities and even incompatible syntax changes, constitute separate mediums? What is useful about applying the term medium here, other than it enables us to discuss the prolific implementation of regular expressions as an example of 'remediation'?</p
	><p
	>Programming languages often borrow concepts from each other, as this example of regular expressions clearly demonstrates. Saying that Perl remediates C syntax because it uses curly braces and semi-colons under-emphasizes Perl's own syntax. Rather it seems more evocative to describe ways in which Perl hybridizes elements of C's grammar while augmenting them with grammar of its own.</p
	><p
	>In other words, to say that</p
	><pre
	><code
	  >my $variable = &quot;value&quot;;    # defining a variable in Perl
</code
	  ></pre
	><p
	>is a remediation of</p
	><pre
	><code
	  >char[5] variable = &quot;value&quot;; /* defining a variable in C */
</code
	  ></pre
	><p
	>is an over-simplification. It obfuscates significant algorithmic differences in the two approaches by focusing on the surface level syntax (which is relatively similar) over the significant internal differences in the way the two languages deal with variables (such as static versus dynamic typing). The grammar of C is hybridized by Perl---re-implemented rather than remediated, related yet irreconcilable. Implementation differences have huge implications on the utility and functionality of the languages, a theoretical framework that focuses on surface-level similarities is incapable of expressing the variation that occurs beneath those similarities.</p
	><p
	>Rather than a remediation of regular expressions, then, we see a hybridization of specific grammars of regular expressions, with the most popularly hybridized grammar deriving from the version found in Perl. However, many of the languages that hybridize the Perl version of regular expressions only implement a particular subset of that version. Additionally, extensions may be added that are not included in Perl. The result is a proliferation of regex--in the programmer shorthand for 'regular expression'--grammars as they are integrated into various process hybridities such as programming languages, command line utilities, and text editors.</p
	><p
	>The website <em
	  >Rubular</em
	  > stands as an example of how far-reaching the hybridization of regular expressions has come in terms of process assemblage complexity (Lovitt 2010). The website utilizes not only the HTTP protocol that drives the World Wide Web, it uses AJAX in order to provide real-time representations of pattern matching within a Ruby interpreter (of which there are many). The GUI browser is involved in this assemblage by design<sup
	  ><a href="#fn12" class="footnoteRef" id="fnref12"
	    >12</a
	    ></sup
	  >. So is a web framework of some kind, from the looks of it the increasingly ubiquitous Ruby on Rails There could be an argument made against such far-reaching hybridity: Ruby can be programmed interactively, line by line, in it's interpreter. The layers of code wrapped around the processing of Ruby regexes could be seen as superfluous---in fact, this is a common attitude of certain hacker types who look with disdain upon any non-essential fuctionality. Questions of <em
	  >essentiality</em
	  > in software remain an under-discussed topic in new media studies, despite the ever-present debates among developers on the issue.</p
	><blockquote>Like calculus (which McLuhan considered a conquest of the tactile area of numbers) regular expressions anticipate the unpredictable and bring repeatability to the immeasurable. A simple * (which means &quot;zero or more of the preceding item&quot;) compresses everything from zero to infinity into a calculable scheme. (Oram 2002)</blockquote>
<p
	>Regular expressions</p
	><p
	>&quot;while text parses and subdivides thought, * dissolves and absorbs all text. Gutenberg separated oral speech into figure and ground, but * combines them again. Like the electron in its post-Newtonian atom shell, * ranges freely and resides nowhere&quot; (Oram 2002).</p
	></div><div class='chapter'></div
      ></div
    ></div
  ></div
><div id="conclusion"
><h1
  ><a href="#TOC"
    ><span class="header-section-number"
      >7</span
      > Conclusion</a
    ></h1
  ></div
><div id="bibliography"
><h1
  ><a href="#TOC"
    ><span class="header-section-number"
      >8</span
      > Bibliography</a
    ></h1
  ><p
  >This is where all the words will go.</p
  ><p
  > tufte</p
  ></div
><div class="footnotes"
><hr
   /><ol
  ><li id="fn1"
    ><p
      >Though information visualization, or 'infoviz', is perhaps the highest profile form of generative design, the size and scope of this practice places it outside the bounds of what can reasonably be addressed here in specifics. As a form of generative design, however, the conclusions reached regarding the parent subject should be applicable to infoviz as well. <a href="#fnref1" class="footnoteBackLink" title="Jump back to footnote 1">↩</a></p
      ></li
    ><li id="fn2"
    ><p
      >Where aesthetics is used &quot;to speak of the organization and management of sensation and perception&quot; (Rossiter 2007: 166). <a href="#fnref2" class="footnoteBackLink" title="Jump back to footnote 2">↩</a></p
      ></li
    ><li id="fn3"
    ><p
      >That is, the 'being of becoming.' <a href="#fnref3" class="footnoteBackLink" title="Jump back to footnote 3">↩</a></p
      ></li
    ><li id="fn4"
    ><p
      >In the video documentary series <em
	>Triumph of the Nerds</em
	>, Steve Jobs reports that &quot;they showed me really three things. But I was so blinded by the first one I didn't even really see the other two&quot; (PBS 1996). In his explanation he was so excited about the demonstration of PARC's graphical user interface that he was unable to absorb the importance of either the Smalltalk-80 object-oriented programming environment or the Ethernet networking technology. <a href="#fnref4" class="footnoteBackLink" title="Jump back to footnote 4">↩</a></p
      ></li
    ><li id="fn5"
    ><p
      >In the interest of full disclosure, at times I edited the Markdown source file on Windows XP computers at my university. <a href="#fnref5" class="footnoteBackLink" title="Jump back to footnote 5">↩</a></p
      ></li
    ><li id="fn6"
    ><p
      >The use of the little 'o' represents both an aesthetic decision in terms of typographic flow and a (hopefully) non-confrontational means of representing my weighing in on the ideological differences between free and open source software development. <a href="#fnref6" class="footnoteBackLink" title="Jump back to footnote 6">↩</a></p
      ></li
    ><li id="fn7"
    ><p
      >'Ideological computing' is phrased as a counterpoint to Manovich's elaboration of a 'cultural computing' in <em
	>Software Takes Command</em
	> (Manovich 2008). <a href="#fnref7" class="footnoteBackLink" title="Jump back to footnote 7">↩</a></p
      ></li
    ><li id="fn8"
    ><p
      >Initially the focus was simply on converting to HTML, though it has since been used to produce a multitude of output formats, as we will soon see. <a href="#fnref8" class="footnoteBackLink" title="Jump back to footnote 8">↩</a></p
      ></li
    ><li id="fn9"
    ><p
      >The language of chemistry was likewise appropriated for the term 'interface' (Cramer and Fuller 2008: 149). <a href="#fnref9" class="footnoteBackLink" title="Jump back to footnote 9">↩</a></p
      ></li
    ><li id="fn10"
    ><p
      >The second edition of the book, published in 2001, was used for this thesis. While it is updated to include the Web, its roots in a significantly older text are worth noting. <a href="#fnref10" class="footnoteBackLink" title="Jump back to footnote 10">↩</a></p
      ></li
    ><li id="fn11"
    ><p
      >Almost twenty years later, layout processors have been for the most part subsumed by word processors. The layout shifting process has been hybridized into the increasingly feature-ful assemblages of word processors where it has not morphed into a niche proprietary product. <a href="#fnref11" class="footnoteBackLink" title="Jump back to footnote 11">↩</a></p
      ></li
    ><li id="fn12"
    ><p
      >A zen thought for the 21^st century: Does a website truly exist if there is no browser to render it? <a href="#fnref12" class="footnoteBackLink" title="Jump back to footnote 12">↩</a></p
      ></li
    ></ol
  ></div
>
</div> <!-- for the chain of chapter divs -->
</div>
</body>
</html>

